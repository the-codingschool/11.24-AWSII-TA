{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1PupwQQp6LZFfMIPgWLKlWXrYDgLuA5sj","timestamp":1678127175525}],"collapsed_sections":["ZZPhj3RW68Kq","Yt03nQ5Z7D2u","PSVTgBD67QFQ","K7jj8YPZ7hFR","G2xgieYx-BIy","zuJwVtiL-G0y","_iLEJ5_vAUzH","ZbhgBwEPCm8P","fuGPjq1bLSNA","JSOOynZqL0xo","M__-6lOuE3nc","7dzC09dLlEhm"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# **Lab 7: Introduction to Natural Language Processing (NLP)**\n","---\n","\n","### **Description**\n","In today's lab, we will see how to use neural networks for one of the most popular NLP tasks: **text classification**. This will involve applying what you already know about neural nets and new NLP concepts of tokenization and vectorization.\n","\n","For this project, we will be working with the `fetch_20newsgroups` dataset, which is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. Each newsgroup covers a different topic, such as sports, politics, religion, and technology. The documents within each newsgroup were posted by various authors, and cover a wide range of subtopics related to the main theme of the newsgroup.\n","\n","The goal of this project is to build a machine learning model that can accurately classify newsgroup documents based on their content.\n","\n","<br>\n","\n","### **Lab Structure**\n","**Part 1**: [Tokenization and Vectorization](#p1)\n","\n","**Part 2**: [News Group Classification with a Neural Network](#p2)\n",">\n",">**Part 2.1**: [Tokenizing and Vectorizing the News Groups Dataset](#p2.1)\n",">\n",">**Part 2.2**: [Training and Testing a Neural Network](#p2.2)\n","\n","**Part 3**: [News Group Classification with a CNN](#p3)\n","\n","\n","\n","<br>\n","\n","### **Goals**\n","By the end of this lab, you will:\n","* Understand the concept of tokenization in NLP.\n","* Compare a fully connected network to a CNN for text classification.\n","\n","<br>\n","\n","### **Cheat Sheets**\n","[Natural Language Processing I](https://docs.google.com/document/d/1MamYMxe8zlWoiDc0tX2RzUKQULCPVUh-2QtdzRRvzcs/edit?usp=sharing)\n","\n","<br>\n","\n","**Before starting, run the code below to import all necessary functions and libraries.**\n"],"metadata":{"id":"mbZXQ3rA3NwL"}},{"cell_type":"code","source":["from sklearn.datasets import fetch_20newsgroups\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from fastai.text.all import *\n","\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"YAvvLhRIoqYp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"p1\"></a>\n","\n","---\n","## **Part 1: Tokenization and Vectorization**\n","---\n","\n","**Run the cell below to load a simple corpus for us to work with.**"],"metadata":{"id":"_X9DchFl6uZg"}},{"cell_type":"code","source":["# Define a collection of text documents\n","corpus = [\n","    \"This is the first document.\",\n","    \"This is the second document.\",\n","    \"And this is the third document.\",\n","    \"Is this the first document?\",\n","]"],"metadata":{"id":"jxb7Iynu6q2a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #1.1: Create a CountVectorizer object**\n","\n"],"metadata":{"id":"ZZPhj3RW68Kq"}},{"cell_type":"code","source":["vectorizer = #FILL IN CODE HERE"],"metadata":{"id":"xtKR2kbk8zpw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### **Solution**"],"metadata":{"id":"-IGvNizM80IG"}},{"cell_type":"code","source":["# Create a CountVectorizer object\n","vectorizer = CountVectorizer()"],"metadata":{"id":"8U1B0rv064r7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #1.2: Fit the vectorizer to the corpus**\n","\n"],"metadata":{"id":"Yt03nQ5Z7D2u"}},{"cell_type":"code","source":[],"metadata":{"id":"ISVa9A52832F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### **Solution**"],"metadata":{"id":"5lXd4kXK832G"}},{"cell_type":"code","source":["# Fit the vectorizer to the corpus\n","vectorizer.fit(corpus)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":75},"id":"G2WTRbt87DM3","executionInfo":{"status":"ok","timestamp":1682300654898,"user_tz":300,"elapsed":17,"user":{"displayName":"Natasha Sachdeva","userId":"11247654195558503951"}},"outputId":"454dce9e-8067-499b-8c8f-08f3d90ae0e1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["CountVectorizer()"],"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["#### **Problem #1.3: Transform the corpus into a matrix of token counts**\n","\n"],"metadata":{"id":"PSVTgBD67QFQ"}},{"cell_type":"code","source":["# Transform the corpus into a matrix of token counts\n","# WRITE YOUR CODE HERE\n","\n","# Print the resulting matrix\n","print(X.toarray())"],"metadata":{"id":"mxfzW9c484nz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### **Solution**"],"metadata":{"id":"PoawSh2g84n0"}},{"cell_type":"code","source":["# Transform the corpus into a matrix of token counts\n","X = vectorizer.transform(corpus)\n","\n","# Print the resulting matrix\n","print(X.toarray())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Lv3xPVi7PjO","executionInfo":{"status":"ok","timestamp":1682300662337,"user_tz":300,"elapsed":161,"user":{"displayName":"Natasha Sachdeva","userId":"11247654195558503951"}},"outputId":"62998035-98c4-4f5a-9202-0f040fea79bd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0 1 1 1 0 1 0 1]\n"," [0 1 0 1 1 1 0 1]\n"," [1 1 0 1 0 1 1 1]\n"," [0 1 1 1 0 1 0 1]]\n"]}]},{"cell_type":"markdown","source":["#### **Problem #1.4: Print the tokens**\n","\n","Use `get_feature_names_out()` to print the tokens.\n"],"metadata":{"id":"K7jj8YPZ7hFR"}},{"cell_type":"code","source":[],"metadata":{"id":"11eK9s-y86TR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### **Solution**"],"metadata":{"id":"nHY0smDe86TS"}},{"cell_type":"code","source":["print(vectorizer.get_feature_names_out())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iuoXE46T7oQg","executionInfo":{"status":"ok","timestamp":1682300755519,"user_tz":300,"elapsed":6,"user":{"displayName":"Natasha Sachdeva","userId":"11247654195558503951"}},"outputId":"dbad7c30-8f44-4eca-db71-d0246e1ce1ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['and' 'document' 'first' 'is' 'second' 'the' 'third' 'this']\n"]}]},{"cell_type":"markdown","source":["Compare the tokens, the matrix, and the corpus. Do you see how each sentence is represented in the matrix?"],"metadata":{"id":"4d_cmQjP7xKT"}},{"cell_type":"markdown","source":["<a name=\"p2\"></a>\n","\n","---\n","## **Part 2: News Group Classification with a Neural Network**\n","---\n","\n","\n","The 20 Newsgroups dataset is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. Our task is to classify the articles to the correct newsgroup.\n","\n","<br>\n","\n","**Run the cell below to load the dataset.**"],"metadata":{"id":"kV7du9rJBYuz"}},{"cell_type":"code","source":["# Load the dataset\n","newsgroups_data = fetch_20newsgroups(\n","    subset='train',\n","    remove=('headers', 'footers', 'quotes')\n",")\n","\n","texts = newsgroups_data.data\n","labels = newsgroups_data.target\n","\n","# Split the dataset into training and validation sets\n","texts_train, texts_val, labels_train, labels_val = train_test_split(\n","    texts,\n","    labels,\n","    test_size=0.2,\n","    random_state=42\n",")"],"metadata":{"id":"foasGD-f9a_R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"p2.1\"></a>\n","\n","---\n","### **Part 2.1: Tokenizing and Vectorizing the News Groups Dataset**\n","---"],"metadata":{"id":"ZqaEmJy38VkC"}},{"cell_type":"markdown","source":["#### **Problem #2.1.1: Create the CountVectorizer object**\n","\n","Initialize the vectorizer with the following parameters:\n","* `stop_words='english'`\n","* `max_features=4000`"],"metadata":{"id":"imQT4CJS9nxb"}},{"cell_type":"code","source":[],"metadata":{"id":"WzdtPvyx-ew7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### **Solution**\n"],"metadata":{"id":"FajTQEOG-ew8"}},{"cell_type":"code","source":["vectorizer = CountVectorizer(stop_words='english', max_features=4000)"],"metadata":{"id":"V0Jm7z3Z-LlG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #2.1.2: Fit and transform the training data.**\n","\n"],"metadata":{"id":"G2xgieYx-BIy"}},{"cell_type":"code","source":[],"metadata":{"id":"-AAplhm8-fQa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### **Solution**\n"],"metadata":{"id":"JcY1bWQf-fQa"}},{"cell_type":"code","source":["X_train_bow = vectorizer.fit_transform(texts_train)"],"metadata":{"id":"HyOslyuK-L9O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #2.1.3: Transform the validation data.**"],"metadata":{"id":"zuJwVtiL-G0y"}},{"cell_type":"code","source":[],"metadata":{"id":"kmk0hhfd-dbz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### **Solution**\n"],"metadata":{"id":"Z5jzUynnJU0t"}},{"cell_type":"code","source":["X_valid_bow = vectorizer.transform(texts_val)"],"metadata":{"id":"SVG1Sy7OJU0s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### **Run the code below to print out the shapes of each BoW matrix and a sample of the vocabulary.**"],"metadata":{"id":"KPeFKyUx_RX4"}},{"cell_type":"code","source":["# Show the shape of the BoW matrices\n","print(\"Shape of the training BoW matrix:\", X_train_bow.shape)\n","print(\"Shape of the validation BoW matrix:\", X_valid_bow.shape)\n","\n","print(L(vectorizer.get_feature_names_out()[2000:2100]))"],"metadata":{"id":"xLcL6cn5JU0t","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682306159550,"user_tz":300,"elapsed":627,"user":{"displayName":"Natasha Sachdeva","userId":"11247654195558503951"}},"outputId":"db23e954-fa61-45fb-992b-c5d5b3d897b5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of the training BoW matrix: (9051, 4000)\n","Shape of the validation BoW matrix: (2263, 4000)\n","[array(['ken', 'kent', 'kept', 'kevin', 'key', 'keyboard', 'keys', 'kg',\n","       'kh', 'khf', 'ki', 'kids', 'kill', 'killed', 'killing', 'kind',\n","       'kinds', 'king', 'kingdom', 'kings', 'kit', 'kjz', 'kk', 'km',\n","       'kn', 'knew', 'knife', 'know', 'knowing', 'knowledge', 'known',\n","       'knows', 'koresh', 'kt', 'kurds', 'la', 'lab', 'labor',\n","       'laboratory', 'lack', 'land', 'lane', 'language', 'large',\n","       'largely', 'larger', 'larry', 'larson', 'laser', 'late', 'later',\n","       'latest', 'launch', 'launched', 'launches', 'law', 'laws',\n","       'lawyer', 'lay', 'lc', 'lcs', 'le', 'lead', 'leader', 'leaders',\n","       'leadership', 'leading', 'leads', 'leafs', 'league', 'learn',\n","       'learned', 'learning', 'leave', 'leaves', 'leaving', 'lebanese',\n","       'lebanon', 'led', 'lee', 'left', 'legal', 'legally', 'legislation',\n","       'legitimate', 'lemieux', 'length', 'let', 'lets', 'letter',\n","       'letters', 'letting', 'level', 'levels', 'lewis', 'lg', 'lhz',\n","       'liar', 'lib', 'liberal'], dtype=object)]\n"]}]},{"cell_type":"markdown","source":["#### **Problem #2.1.4: Choose a random document and print out its BoW representation.**\n","\n","Then use `get_feature_names_out()` to determine what some of the words are."],"metadata":{"id":"_iLEJ5_vAUzH"}},{"cell_type":"code","source":["random_doc_idx = # You can choose any index\n","print(\"BoW representation of a random document:\\n\", X_train_bow[random_doc_idx])"],"metadata":{"id":"XOm2QWcxAtEg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use get_feature_names_out() to explore your results"],"metadata":{"id":"eVuwXMMLAtgH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### **Solution**"],"metadata":{"id":"z2-dXvjZAt8Z"}},{"cell_type":"code","source":["random_doc_idx = 42  # You can choose any index\n","print(\"BoW representation of a random document:\\n\", X_train_bow[random_doc_idx])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c_VXw2CT4BvI","executionInfo":{"status":"ok","timestamp":1682306181247,"user_tz":300,"elapsed":150,"user":{"displayName":"Natasha Sachdeva","userId":"11247654195558503951"}},"outputId":"70668057-245d-412d-9a11-94e246aa59d1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["BoW representation of a random document:\n","   (0, 790)\t1\n","  (0, 3060)\t1\n","  (0, 3353)\t1\n","  (0, 2050)\t1\n","  (0, 3136)\t1\n","  (0, 1988)\t1\n","  (0, 2213)\t1\n","  (0, 1460)\t1\n","  (0, 3699)\t1\n","  (0, 860)\t1\n","  (0, 3156)\t2\n","  (0, 553)\t1\n","  (0, 3616)\t1\n","  (0, 1964)\t1\n","  (0, 3893)\t1\n","  (0, 1245)\t1\n","  (0, 3258)\t1\n","  (0, 1969)\t1\n","  (0, 647)\t1\n"]}]},{"cell_type":"code","source":["vectorizer.get_feature_names_out()[3156]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"LAeElevd4K1z","executionInfo":{"status":"ok","timestamp":1682306198863,"user_tz":300,"elapsed":182,"user":{"displayName":"Natasha Sachdeva","userId":"11247654195558503951"}},"outputId":"9c2992f4-7a38-45fe-9ddd-3e0d7ec218cf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'say'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":104}]},{"cell_type":"markdown","source":["<a name=\"p2.2\"></a>\n","\n","---\n","### **Part 2.2: Training and Testing a Neural Network**\n","---\n","\n","At this point, we have imported, split, and vectorized the data. Now we need to prepare it for a PyTorch model and proceed as we would for *any* classification task with a PyTorch model."],"metadata":{"id":"XGMs3RKZI8Of"}},{"cell_type":"markdown","source":["#### **Step #1**\n","\n","**This code has been provided for you. Run the cell below.**"],"metadata":{"id":"eN8Z-C_3K0ZG"}},{"cell_type":"code","source":["# Convert to PyTorch tensors\n","X_train = torch.tensor(X_train_bow.todense()).float()\n","X_valid = torch.tensor(X_valid_bow.todense()).float()\n","\n","# Extract labels\n","y_train = torch.tensor(labels_train)\n","y_valid = torch.tensor(labels_val)\n","\n","# Create DataLoaders\n","train_dataset = list(zip(X_train, y_train))\n","valid_dataset = list(zip(X_valid, y_valid))\n","\n","train_dl = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","val_dl = DataLoader(valid_dataset, batch_size=64)\n","dls = DataLoaders(train_dl,val_dl)"],"metadata":{"id":"liLaJe3dLPN9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Step #2**\n","\n","For a fully connected network, the dimension of the input layer will be the number of tokens. Complete the code below."],"metadata":{"id":"hc5z8AhGCKHt"}},{"cell_type":"code","source":[],"metadata":{"id":"A4H2ukzNCj41"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### **Solution**"],"metadata":{"id":"G5yt5g6fCkSp"}},{"cell_type":"code","source":["input_dims = len(vectorizer.get_feature_names_out())"],"metadata":{"id":"Xnh153Y3Cedo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Steps #3-6**\n","\n","Define a fully connected network of your own design. Ensure you have the correct number of inputs and outputs."],"metadata":{"id":"ZbhgBwEPCm8P"}},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qrIklOq1CmS0","executionInfo":{"status":"ok","timestamp":1682303352977,"user_tz":300,"elapsed":4,"user":{"displayName":"Natasha Sachdeva","userId":"11247654195558503951"}},"outputId":"b3245ac4-4b3a-4628-e7da-8b1a309029e9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["20"]},"metadata":{},"execution_count":40}]},{"cell_type":"markdown","source":["###### **Solution**\n"],"metadata":{"id":"4gQATg_-LROR"}},{"cell_type":"code","source":["# Define the DNN model\n","dnn_model = nn.Sequential(\n","    nn.Linear(input_dims, 512),\n","    nn.ReLU(),\n","    nn.Linear(512, 20)\n",")"],"metadata":{"id":"eBLxBovbFO8A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Step #7**\n","\n","Create a Learner object and fit the model. Since this is a multiclass classification problem, you will use `nn.CrossEntropyLoss()`"],"metadata":{"id":"fuGPjq1bLSNA"}},{"cell_type":"code","source":["# Create a Learner and train the model\n"],"metadata":{"id":"rjiBv6JzLk4C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### **Solution**\n"],"metadata":{"id":"xL3OlsDjLzsE"}},{"cell_type":"code","source":["# Create a Learner and train the model\n","dnn_learner = Learner(\n","    dls,\n","    dnn_model,\n","    loss_func=nn.CrossEntropyLoss(),\n","    metrics=accuracy)\n","\n","dnn_learner.fit(5, 1e-2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"f3FnosgSESX4","executionInfo":{"status":"ok","timestamp":1682306236929,"user_tz":300,"elapsed":24575,"user":{"displayName":"Natasha Sachdeva","userId":"11247654195558503951"}},"outputId":"c87facfb-15af-405e-e35f-509984f98fed"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<style>\n","    /* Turns off some styling */\n","    progress {\n","        /* gets rid of default border in Firefox and Opera. */\n","        border: none;\n","        /* Needs to be in here for Safari polyfill so background images work as expected. */\n","        background-size: auto;\n","    }\n","    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n","        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n","    }\n","    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","        background: #F44336;\n","    }\n","</style>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>valid_loss</th>\n","      <th>accuracy</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>1.619478</td>\n","      <td>1.426403</td>\n","      <td>0.624834</td>\n","      <td>00:05</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.907468</td>\n","      <td>1.559543</td>\n","      <td>0.652673</td>\n","      <td>00:04</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.443807</td>\n","      <td>1.858257</td>\n","      <td>0.640301</td>\n","      <td>00:05</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.220311</td>\n","      <td>2.206271</td>\n","      <td>0.627044</td>\n","      <td>00:04</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.192995</td>\n","      <td>2.542234</td>\n","      <td>0.641184</td>\n","      <td>00:04</td>\n","    </tr>\n","  </tbody>\n","</table>"]},"metadata":{}}]},{"cell_type":"markdown","source":["#### **Step #8**\n","\n","Now, evaluate the model for both the training and validation sets.\n"],"metadata":{"id":"JSOOynZqL0xo"}},{"cell_type":"code","source":["# Evaluate the training set\n","\n","\n","# Evaluate the test set\n"],"metadata":{"id":"HqPSJlsSL7_E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### **Solution**\n"],"metadata":{"id":"KhhEcdN2L7Nc"}},{"cell_type":"code","source":["# Calculate training accuracy\n","train_loss, train_accuracy = dnn_learner.validate(dl=dls.train)\n","print(f\"Training accuracy: {train_accuracy:.4f}\")\n","\n","# Calculate validation accuracy\n","valid_loss, valid_accuracy = dnn_learner.validate(dl=dls.valid)\n","print(f\"Validation accuracy: {valid_accuracy:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"GchfUp2aIc83","executionInfo":{"status":"ok","timestamp":1682306241711,"user_tz":300,"elapsed":1694,"user":{"displayName":"Natasha Sachdeva","userId":"11247654195558503951"}},"outputId":"6b8e8223-76bf-4fa1-9124-f0242b173569"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<style>\n","    /* Turns off some styling */\n","    progress {\n","        /* gets rid of default border in Firefox and Opera. */\n","        border: none;\n","        /* Needs to be in here for Safari polyfill so background images work as expected. */\n","        background-size: auto;\n","    }\n","    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n","        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n","    }\n","    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","        background: #F44336;\n","    }\n","</style>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Training accuracy: 0.9619\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<style>\n","    /* Turns off some styling */\n","    progress {\n","        /* gets rid of default border in Firefox and Opera. */\n","        border: none;\n","        /* Needs to be in here for Safari polyfill so background images work as expected. */\n","        background-size: auto;\n","    }\n","    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n","        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n","    }\n","    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","        background: #F44336;\n","    }\n","</style>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Validation accuracy: 0.6412\n"]}]},{"cell_type":"markdown","source":["#### **How did your model perform?**"],"metadata":{"id":"zkn0RBS0DHPo"}},{"cell_type":"markdown","source":["<a name=\"p3\"></a>\n","\n","---\n","## **Part 3: News Group Classification with a CNN**\n","---\n","\n"],"metadata":{"id":"r4_U4kWylKLw"}},{"cell_type":"markdown","source":["#### **Step #1**\n","\n","**The code for importing the data is provided for you. Run the cell below.**"],"metadata":{"id":"BQ6jclf5Po-o"}},{"cell_type":"code","source":["# Convert to PyTorch tensors\n","X_train = torch.tensor(X_train_bow.todense()).float().unsqueeze(1)\n","X_valid = torch.tensor(X_valid_bow.todense()).float().unsqueeze(1)\n","\n","# Extract labels\n","y_train = torch.tensor(labels_train)\n","y_valid = torch.tensor(labels_val)\n","\n","# Create DataLoaders\n","train_dataset = list(zip(X_train, y_train))\n","valid_dataset = list(zip(X_valid, y_valid))\n","\n","train_dl = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","val_dl = DataLoader(valid_dataset, batch_size=64)\n","dls = DataLoaders(train_dl,val_dl)\n","\n","input_dims = len(vectorizer.get_feature_names_out())"],"metadata":{"id":"LY31FfYdP3xa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Steps #3-6**\n","\n","Let's start by building a new CNN model. Remember, the syntax for CNNs for NLP is a little different than for images. We will be using the 1D versions of the convolution and max pooling layers. Examples:\n","* `nn.Conv1d(64, 128, kernel_size=5, padding=2)`\n","* `nn.MaxPool1d(2)`\n","\n","Define a CNN with the following layers:\n","\n","Block 1:\n","* A convolutional layer with the appropriate input dimension and 16 outputs, kernel size of 3, `padding=1`, and ReLU activation.\n","* A max pooling layer with a pool size of 2\n","\n","Block 2\n","* A convolutional layer with 32 outputs, kernel size of 3, `padding=1`, and ReLU activation.\n","* A max pooling layer with a pool size of 2\n","\n","Finally, add:\n","* A linear layer with 8 outputs and ReLU activation\n","* The output layer"],"metadata":{"id":"1AAVJHkuE3nM"}},{"cell_type":"code","source":[],"metadata":{"id":"YVE6HlQ4E3nc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### **Solution**\n"],"metadata":{"id":"SsQVxdIlE3nc"}},{"cell_type":"code","source":["# Define the CNN Model\n","cnn_model = nn.Sequential(\n","    nn.Conv1d(1, 16, kernel_size=3, padding=1),\n","    nn.ReLU(),\n","    nn.MaxPool1d(2),\n","    nn.Conv1d(16, 32, kernel_size=3, padding=1),\n","    nn.ReLU(),\n","    nn.MaxPool1d(2),\n","    nn.Flatten(),\n","    nn.Linear(32 * (input_dims // 4), 8),\n","    nn.ReLU(),\n","    nn.Linear(8, 20)\n",")"],"metadata":{"id":"DhNrWG1ME3nc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Step #7**\n","\n","Create a Learner object and fit the model. Since this is a multiclass classification problem, you will use `nn.CrossEntropyLoss()`"],"metadata":{"id":"M__-6lOuE3nc"}},{"cell_type":"code","source":["# Create a Learner and train the model\n"],"metadata":{"id":"OaB29Xv2E3nd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### **Solution**\n"],"metadata":{"id":"l0lzvEXmE3nd"}},{"cell_type":"code","source":["# Create a Learner and train the model\n","cnn_learner = Learner(\n","    dls,\n","    cnn_model,\n","    loss_func=nn.CrossEntropyLoss(),\n","    metrics=accuracy)\n","\n","cnn_learner.fit(5, 1e-4)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1682306427908,"user_tz":300,"elapsed":123297,"user":{"displayName":"Natasha Sachdeva","userId":"11247654195558503951"}},"outputId":"c8b79d1b-6f27-4c76-ab60-bad3e7ccb65c","id":"1zITkJFUE3nd"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<style>\n","    /* Turns off some styling */\n","    progress {\n","        /* gets rid of default border in Firefox and Opera. */\n","        border: none;\n","        /* Needs to be in here for Safari polyfill so background images work as expected. */\n","        background-size: auto;\n","    }\n","    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n","        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n","    }\n","    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","        background: #F44336;\n","    }\n","</style>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>valid_loss</th>\n","      <th>accuracy</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>1.567503</td>\n","      <td>1.803258</td>\n","      <td>0.447636</td>\n","      <td>00:24</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>1.528314</td>\n","      <td>1.768604</td>\n","      <td>0.464870</td>\n","      <td>00:25</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>1.495133</td>\n","      <td>1.750425</td>\n","      <td>0.470172</td>\n","      <td>00:24</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>1.490543</td>\n","      <td>1.738211</td>\n","      <td>0.473707</td>\n","      <td>00:25</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>1.447800</td>\n","      <td>1.728292</td>\n","      <td>0.479894</td>\n","      <td>00:23</td>\n","    </tr>\n","  </tbody>\n","</table>"]},"metadata":{}}]},{"cell_type":"markdown","source":["#### **Step #8**\n","\n","Now, evaluate the model for both the training and validation sets.\n"],"metadata":{"id":"r5KifHJwE3ne"}},{"cell_type":"code","source":["# Evaluate the training set\n","\n","\n","# Evaluate the test set\n"],"metadata":{"id":"k_8YoV92E3ne"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### **Solution**\n"],"metadata":{"id":"3WwmizSGE3nf"}},{"cell_type":"code","source":["# Calculate training accuracy\n","train_loss, train_accuracy = cnn_learner.validate(dl=dls.train)\n","print(f\"Training accuracy: {train_accuracy:.4f}\")\n","\n","# Calculate validation accuracy\n","valid_loss, valid_accuracy = cnn_learner.validate(dl=dls.valid)\n","print(f\"Validation accuracy: {valid_accuracy:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1682306901172,"user_tz":300,"elapsed":5233,"user":{"displayName":"Natasha Sachdeva","userId":"11247654195558503951"}},"outputId":"18349565-30d9-439e-d148-b6971b79bfb5","id":"39_Uuz7XE3nf"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<style>\n","    /* Turns off some styling */\n","    progress {\n","        /* gets rid of default border in Firefox and Opera. */\n","        border: none;\n","        /* Needs to be in here for Safari polyfill so background images work as expected. */\n","        background-size: auto;\n","    }\n","    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n","        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n","    }\n","    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","        background: #F44336;\n","    }\n","</style>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Training accuracy: 0.5496\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<style>\n","    /* Turns off some styling */\n","    progress {\n","        /* gets rid of default border in Firefox and Opera. */\n","        border: none;\n","        /* Needs to be in here for Safari polyfill so background images work as expected. */\n","        background-size: auto;\n","    }\n","    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n","        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n","    }\n","    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","        background: #F44336;\n","    }\n","</style>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Validation accuracy: 0.4799\n"]}]},{"cell_type":"markdown","source":["**Oh no!** It looks like the CNN didn't do much better! It turns out that tokenization and vectorization is not enough to prepare text data for deep learning. There's an additional processing step we can take that will set our models up for success: **embedding.** We will see how embedding improves model performance in the next lab."],"metadata":{"id":"S6ddmcs4xIKQ"}},{"cell_type":"markdown","source":["# End of notebook\n","---\n","© 2024 The Coding School, All rights reserved"],"metadata":{"id":"7dzC09dLlEhm"}}]}