{"cells":[{"cell_type":"markdown","metadata":{"id":"U8zMXCjs0Rqw"},"source":["# **Lab 6: Deep Learning - Review**\n","\n","---\n","### **Description**\n","In today's lab, you will work on two projects using Deep Learning to classify variations on the MNIST digits dataset that involve more complex imagery. You will be asked to apply what you have learned over the last two days and make choices of your own to complete these projects. We recommend that you have the Days 3 - 4 notebooks and the cheatsheets open as reference so you can look up anything you are unsure of.\n","\n","<br>\n","\n","### **Lab Structure**\n","**Part 1**: [Analysis of the Kuzushiji-MNIST Dataset](#p1)\n","\n","> **Part 1.1**: [The Initial Attempt](#p1.1)\n","\n","> **Part 1.2**: [Improvements](#p1.2)\n","\n","**Part 2**: [Analysis of the MNIST Fashion Dataset](#p2)\n","\n","> **Part 2.1**: [The Initial Attempt](#p2.1)\n","\n","> **Part 2.2**: [Improvements](#p2.2)\n","\n","**Part 3**: [Analysis of Mineral Hardness](#p3)\n","\n","> **Part 3.1**: [The Initial Attempt](#p3.1)\n","\n","> **Part 3.2**: [Improvements](#p3.2)\n","\n","**Part 4**: [Analysis of the Country211 Dataset](#p4)\n","\n","> **Part 4.1**: [The Initial Attempt](#p4.1)\n","\n","> **Part 4.2**: [Improvements](#p4.2)\n","\n","<br>\n","\n","\n","\n","\n","### **Learning Objectives**\n","By the end of this lab, you will have trained, implemented, and evaluated a deep learning image classification model that is able to achieve over 85% accuracy on the validation dataset.\n","\n","<br>\n","\n","\n","### **Cheat Sheets**\n","* [Deep Learning with pytorch](https://docs.google.com/document/d/1Wm01maZUrSuwdOhuI05uZBtqt5nL5shOGnJ7kTHWl_I/edit?usp=drive_link)\n","\n","<br>\n","\n","**Before starting, run the code below to import all necessary functions and libraries.**\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"N-h39eV6JvTZ","tags":[],"executionInfo":{"status":"ok","timestamp":1730425255695,"user_tz":420,"elapsed":20709,"user":{"displayName":"Angelique Membrido","userId":"04984385250682432912"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from fastai.vision.all import *\n","from torchvision.datasets import KMNIST\n","from torch.utils.data import DataLoader\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","import matplotlib.pyplot as plt\n","import torchvision.transforms as transforms\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","metadata":{"id":"pvPwgOD3ptUe"},"source":["<a name=\"p1\"></a>\n","\n","---\n","## **Part 1: Analysis of the Kuzushiji-MNIST Dataset**\n","---\n","\n","![Kanji](https://raw.githubusercontent.com/rois-codh/kmnist/master/images/kkanji_examples.png)\n","\n","(Image taken from the Kuzushiji-MNIST dataset.)\n","\n","<br>\n","\n","The `K-MNIST` dataset is a newer MNIST-like dataset containing 10 phonetic letters of Hiragana, a Japanese syllabary and component of the Japanese writing system. One intention of the dataset was to link Hiragana from classical literature to modern counterparts (UTF-8 encoded).\n","\n","Using deep learning, you will develop and evaluate various neural networks to train an AI in image recognition. This project will challenge you to design your own neural networks and evaluate how your choices of hyperparameters impacted your models' accuracies.\n","\n","<br>\n","\n","**Your goal is to create a model that classifies the Hirgana letters from the validation dataset with 85% accuracy or more.**"]},{"cell_type":"markdown","metadata":{"id":"UyyNtTjHtIP4"},"source":["<a name=\"p1.1\"></a>\n","\n","---\n","### **Part 1.1: The Initial Attempt**\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"kDVoAJYtfjlI"},"source":["#### **Step #1: Import and split the dataset into a train/test set**\n","\n","\n","**Run the code below to load the dataset.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JMzgoxJ8tIuT","tags":[]},"outputs":[],"source":["# Define the transformations\n","transform = transforms.Compose([\n","    transforms.Resize((28, 28)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.1307,), (0.3081,))\n","])\n","\n","# Load the KMNIST dataset\n","train_dataset = KMNIST(root='./data', train=True, download=True, transform=transform)\n","valid_dataset = KMNIST(root='./data', train=False, download=True, transform=transform)\n","\n","# Create the DataLoaders object\n","train_dl = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","valid_dl = DataLoader(valid_dataset, batch_size=64)\n","dls = DataLoaders(train_dl, valid_dl)\n","\n","# Set the number of images per row and column in the grid\n","n_row = 4\n","n_col = 4\n","\n","# Get a batch of training data\n","images, labels = next(iter(train_dl))\n","\n","# Create a grid of images and labels\n","fig, axs = plt.subplots(n_row, n_col, figsize=(9, 9))\n","for i in range(n_row):\n","    for j in range(n_col):\n","        ax = axs[i, j]\n","        img_idx = i * n_col + j\n","        img = images[img_idx].reshape(28, 28).numpy()  # Reshape the image to 28x28\n","        label = labels[img_idx].item()\n","        ax.imshow(img, cmap='gray')\n","        ax.set_title(f\"Label: {label}\")\n","        ax.axis('off')\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"_902bpyuftd5"},"source":["#### **Step #2: Determine the dimensions of the data**\n","\n","\n","Determine the number of input neurons and if the data needs to be flattened."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DSwdSHDN9W6X"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"MGASEdns947X"},"source":["#### **Steps #3 - 6: Build the Model**\n","\n","\n","Build a model sequentially such that there is:\n","\n","* The Input Layer, flattened if necessary, correctly sized for the input\n","* No hidden layers\n","* The Output Layer with the correct number of neurons."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kjBnMony-g-h"},"outputs":[],"source":["# Define the neural network architecture\n","model = nn.Sequential(\n","    # ADD YOUR LAYERS HERE\n",")"]},{"cell_type":"markdown","metadata":{"id":"4bmz7ccjDXAa"},"source":["#### **Step #7: Fit the model**\n","\n","Fitting our model has three sub-steps:\n","\n","**1. Define the loss function.** Since this is a classification problem, we will be using `nn.CrossEntropyLoss()`.\n","\n","<br>\n","\n","**2. Create the Learner object.** Provide the DataLoaders, model, loss function, and use the accuracy metric.\n","\n","<br>\n","\n","**3. Fit the model with the data: Set the number of training epochs and the batch size.** We would recommend starting with a small number of epochs (5) and increasing as necessary. A learning rate of 0.001 is a good starting value."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fzTzSQI9DdZ4"},"outputs":[],"source":["# Define the loss function\n","\n","\n","# Create the Learner object\n","\n","\n","# Train the model\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Pi6ijIAiEFJ1"},"source":["#### **Step #8: Evaluate the model**\n","\n","\n","Print the model's accuracy on both the training and validation sets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eavCx9WpEGza"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"TqE-sclhEVX-"},"source":["#### **Step #9: Visualize the model's predictions**\n","\n","\n","Now, run the code below to make predictions with the model and output its predictions for digits in the validation set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QNX4aeiuEXhR","tags":[]},"outputs":[],"source":["# Get a batch of validation data\n","images, labels = next(iter(valid_dl))\n","\n","# Get the model's predictions for the batch\n","with torch.no_grad():\n","    outputs = model(images)\n","    pred_labels = torch.argmax(outputs, dim=1)\n","\n","# Set the number of images per row and column in the grid\n","n_row = 4\n","n_col = 4\n","\n","# Create a grid of images and labels\n","fig, axs = plt.subplots(n_row, n_col, figsize=(9, 9))\n","for i in range(n_row):\n","    for j in range(n_col):\n","        ax = axs[i, j]\n","        img_idx = i * n_col + j\n","        img = images[img_idx].reshape(28, 28).numpy()  # Reshape the image to 28x28\n","        true_label = labels[img_idx].item()\n","        pred_label = pred_labels[img_idx].item()\n","        ax.imshow(img, cmap='gray')\n","        ax.set_title(f\"True: {true_label}\\nPred: {pred_label}\", fontsize=12)\n","        ax.axis('off')\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"vzDgGv6VKhdL"},"source":["<a name=\"p1.2\"></a>\n","\n","---\n","### **Part 1.2: Improvements**\n","---\n","\n","\n","Using the template cell provided below, create and train a neural network that is capable of achieving an 85% or higher accuracy on the validation dataset for classifying the Hiragana letters.\n","\n","**Hint:** The first step to improving your work above is adding at least one Hidden Layer. However, you can also consider:\n","* Changing the number of neurons in each Hidden Layer\n","* Changing the activation functions in the Hidden Layers\n","* Changing the activation function in the Output Layer\n","* Training with a different learning rate\n","* Changing the number of training epochs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"AVVp7ci1cX3g"},"outputs":[],"source":["# BUILD\n","#=======\n","# Define the neural network architecture\n","model = nn.Sequential(\n","    # ADD YOUR LAYERS HERE\n",")\n","\n","\n","\n","# FIT\n","#=====\n","# Define the loss function\n","\n","\n","# Create the Learner object\n","\n","\n","# Train the model\n","\n","\n","\n","\n","# EVALUATE\n","#==========\n","# Calculate training accuracy\n","train_loss, train_accuracy = # WRITE YOUR CODE HERE\n","print(f\"Training accuracy: {train_accuracy:.4f}\")\n","\n","# Calculate validation accuracy\n","valid_loss, valid_accuracy = # WRITE YOUR CODE HERE\n","print(f\"Validation accuracy: {valid_accuracy:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"exeiT49Ji56A"},"source":["\n","Congratulations! You have completed the project. You should now know how to:\n","\n","> Train, implement, and evaluate a deep learning image recognition model that is able to label images from the K-MNIST dataset with over 85% accuracy.  \n","\n"]},{"cell_type":"markdown","source":["---\n","\n","<center>\n","\n","### **Wait for Your Instructor to Continue**\n","\n","---"],"metadata":{"id":"3v9vWRoX-dqf"}},{"cell_type":"markdown","metadata":{"id":"70sYML6ANO5H"},"source":["<a name=\"p2\"></a>\n","\n","---\n","## **Part 2: Analysis of the MNIST Fashion Dataset**\n","---\n","\n","\n","![fashion_mnist](https://4.bp.blogspot.com/-OQZGt_5WqDo/Wa_Dfa4U15I/AAAAAAAAAUI/veRmAmUUKFA19dVw6XCOV2YLO6n-y_omwCLcBGAs/s1600/out.jpg)\n","\n","(Image taken from the MNIST Fashion dataset.)\n","\n","<br>\n","\n","The MNIST Fashion dataset is a newer MNIST-like dataset containing images of Zalando's articles. Each training and test example is assigned to one of the following 10 labels: t-shirt/top, trouser, pullover, dress, coat, sandal, shirt, sneaker, bag, and ankle boot. Zalando sought to replace the original MNIST dataset.\n","\n","Using deep learning, you will develop and evaluate various neural networks to train an AI in image recognition. This project will continue to challenge you to design your own neural networks and evaluate how your choices of hyperparameters impacted your models' accuracies.\n","\n","<br>\n","\n","**Your goal is to create a model that classifies the fashion items from the validation dataset with 85% accuracy or more.**"]},{"cell_type":"markdown","metadata":{"id":"d4C-hUwnO9Kc"},"source":["<a name=\"p2.1\"></a>\n","\n","---\n","### **Part 2.1: The Initial Attempt**\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"qJaMN6GO2SSj"},"source":["#### **Step #1: Import and split the dataset into a train/test set**\n","\n","**Run the code below to import, split, and visualize the data.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B09Iw_m82c4q","tags":[]},"outputs":[],"source":["from torchvision.datasets import FashionMNIST\n","\n","# Define the transformations\n","transform = transforms.Compose([\n","    transforms.Resize((28, 28)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.2860,), (0.3530,))\n","])\n","\n","# Load the MNIST fashion dataset\n","train_dataset = FashionMNIST(root='./data', train=True, download=True, transform=transform)\n","valid_dataset = FashionMNIST(root='./data', train=False, download=True, transform=transform)\n","\n","# Create the DataLoaders object\n","train_dl = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","valid_dl = DataLoader(valid_dataset, batch_size=64)\n","dls = DataLoaders(train_dl, valid_dl)\n","\n","# Set the number of images per row and column in the grid\n","n_row = 4\n","n_col = 4\n","\n","# Get a batch of training data\n","images, labels = next(iter(train_dl))\n","\n","# Create a grid of images and labels\n","fig, axs = plt.subplots(n_row, n_col, figsize=(9, 9))\n","for i in range(n_row):\n","    for j in range(n_col):\n","        ax = axs[i, j]\n","        img_idx = i * n_col + j\n","        img = images[img_idx].reshape(28, 28).numpy()  # Reshape the image to 28x28\n","        label = labels[img_idx].item()\n","        ax.imshow(img, cmap='gray')\n","        ax.set_title(f\"Label: {label}\")\n","        ax.axis('off')\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"-p1PtkoCO9Kc"},"source":["#### **Step #2: Determine the dimensions of the data**\n","\n","\n","**Determine the number of input neurons and if the data needs to be flattened.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5TdJjxjnO9Kc"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"jdG7OGSGO9Kd"},"source":["#### **Steps #3 - 6: Build the Model**\n","\n","\n","Build a model sequentially such that there is:\n","\n","* The Input Layer, flattened if necessary, and correctly sized for receiving inputs.\n","* No Hidden Layers\n","* The Output Layer with the correct number of neurons."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k-Ul0PMpO9Kd"},"outputs":[],"source":["# Define the neural network architecture\n","model = nn.Sequential(\n","    # ADD YOUR LAYERS HERE\n",")"]},{"cell_type":"markdown","metadata":{"id":"mQa6davTO9Kd"},"source":["#### **Step #7: Fit the model**\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o83F8f7yO9Kd"},"outputs":[],"source":["# Define the loss function\n","\n","# Create the Learner object\n","\n","# Train the model\n"]},{"cell_type":"markdown","metadata":{"id":"YNUc7qPKO9Ke"},"source":["#### **Step #8: Evaluate the model**\n","\n","\n","Print the model's accuracy on both the training and test sets as percentages."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RzSNw0-KO9Ke"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"q6EAad3tO9Ke"},"source":["#### **Step #9: Visualize the model's predictions**\n","\n","\n","Now, run the code below to make predictions with the model and output its predictions for items in the validation set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HCs1s7dCO9Ke"},"outputs":[],"source":["# Get a batch of validation data\n","val_images, val_labels = next(iter(valid_dl))\n","\n","# Get model predictions for the validation batch\n","val_preds = learn.get_preds(dl=[(val_images, val_labels)])\n","val_pred_labels = torch.argmax(val_preds[0], dim=1)\n","\n","# Set the number of images per row and column in the grid\n","n_row = 4\n","n_col = 4\n","\n","# Create a grid of images and labels with predictions\n","fig, axs = plt.subplots(n_row, n_col, figsize=(9, 9))\n","for i in range(n_row):\n","    for j in range(n_col):\n","        ax = axs[i, j]\n","        img_idx = i * n_col + j\n","        img = val_images[img_idx].reshape(28, 28).numpy()  # Reshape the image to 28x28\n","        true_label = val_labels[img_idx].item()\n","        pred_label = val_pred_labels[img_idx].item()\n","        ax.imshow(img, cmap='gray')\n","        ax.set_title(f\"True label: {true_label}\\nPredicted label: {pred_label}\")\n","        ax.axis('off')\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"ay6I6IocO9Ke"},"source":["<a name=\"p2.2\"></a>\n","\n","---\n","### **Part 2.2: Improvements**\n","---\n","\n","\n","Using the template cell provided below, create and train a neural network that is capable of achieving an 85% or higher accuracy on the test dataset for classifying the fashion items.\n","\n","**Hint:** The first step to improving your work above is adding at least one Hidden Layer. However, you can also consider:\n","* Changing the number of neurons in each Hidden Layer\n","* Changing the activation functions in the Hidden Layers\n","* Changing the activation function in the Output Layer\n","* Training with a different optimizer\n","* Training with a different learning rate\n","* Changing the number of training epochs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"vce40xK1O9Ke"},"outputs":[],"source":["# BUILD\n","#=======\n","# Define the neural network architecture\n","model = nn.Sequential(\n","    # ADD YOUR LAYERS HERE\n",")\n","\n","\n","\n","# FIT\n","#=====\n","# Define the loss function\n","\n","\n","# Create the Learner object\n","\n","\n","# Train the model\n","\n","\n","\n","\n","# EVALUATE\n","#==========\n","# Calculate training accuracy\n","train_loss, train_accuracy = # WRITE YOUR CODE HERE\n","print(f\"Training accuracy: {train_accuracy:.4f}\")\n","\n","# Calculate validation accuracy\n","valid_loss, valid_accuracy = # WRITE YOUR CODE HERE\n","print(f\"Validation accuracy: {valid_accuracy:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"f1QJ5k963vzc"},"source":["Congratulations! You have completed the project. You should now know how to:\n","\n","* Train, implement, and evaluate a deep learning image recognition model that is able to label images from the MNIST-Fashion dataset with over 85% accuracy.\n","\n","\n","<br>\n","\n","But, what's the **big picture** idea here? Well, let's think. What patterns did your analysis exhibit?\n","\n","One thing that might be worth noting is that for each of the models that you generated and evaluated, the time it took for the model to finish training was quite lengthy.\n","\n","Now, our datasets are relatively smaller compared to other deep learning implementations. **Access to memory resources and time pose challenges to neural network models** Are there ways of getting around this?\n","\n","Yes and no, but that's okay! There do exist resources such as GPUs and network architectures like convolutional neural networks that allow us to make the best of modern capabilities. However, this is where a lot of science and AI research is currently at! Understanding and improving our use of neural networks.\n","\n","You're now equipped with the tools to start exploring the world of neural networks and deep learning! **What will you train next?**"]},{"cell_type":"markdown","metadata":{"id":"o11oO9o98rUe"},"source":["<a name=\"p3\"></a>\n","\n","---\n","## **[ADDITIONAL PRACTICE] Part 3: Analysis of Mineral Hardness**\n","---\n","\n","Determining the hardness of minerals is vital for designing materials that may use them. In this section, you will build a regression model to predict the hardness of minerals based on their atomic properties.\n"]},{"cell_type":"markdown","metadata":{"id":"E-ymZtj-_DII"},"source":["<a name=\"p3.1\"></a>\n","\n","---\n","### **Part 3.1: The Initial Attempt**\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"MAgo_JDT_DIT"},"source":["#### **Step #1: Import and split the dataset into a train/test set**\n","\n","**Run the code below to import, split, and visualize the data.**"]},{"cell_type":"code","source":["mineral_df = pd.read_csv('https://raw.githubusercontent.com/the-codingschool/TRAIN-datasets/main/material%20Mohs%20hardness/Mineral_Dataset.csv').drop(columns = 'Unnamed: 0')\n","mineral_df.head()\n","\n","X = mineral_df.drop('Hardness', axis = 1)\n","y = mineral_df['Hardness']\n","\n","# Split the data into training and validation sets\n","X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Convert the numpy arrays to PyTorch tensors with float32 data type\n","X_train = torch.tensor(X_train.values, dtype=torch.float32)\n","y_train = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n","\n","X_valid = torch.tensor(X_valid.values, dtype=torch.float32)\n","y_valid = torch.tensor(y_valid.values, dtype=torch.float32).unsqueeze(1)\n","\n","# Create dataset object\n","train_ds = list(zip(X_train, y_train))\n","valid_ds = list(zip(X_valid, y_valid))\n","\n","# Define the DataLoaders\n","train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n","valid_dl = DataLoader(valid_ds, batch_size=64, shuffle=True)\n","\n","dls = DataLoaders(train_dl, valid_dl)"],"metadata":{"id":"4tlu3ZIF0GBd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gUuWmduM_DIU"},"source":["#### **Step #2: Determine the dimensions of the data**\n","\n","\n","**Determine the number of input neurons and if the data needs to be flattened.**"]},{"cell_type":"code","source":[],"metadata":{"id":"uxP_tP2M_DIU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FVTOXhlX_DIU"},"source":["#### **Steps #3 - 6: Build the Model**\n","\n","\n","Build a model sequentially such that there is:\n","\n","* The Input Layer, flattened if necessary, and correctly sized for receiving inputs.\n","* No Hidden Layers\n","* The Output Layer with the correct number of neurons such that we are predicting a continuous number."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EUOFACOO_DIV"},"outputs":[],"source":["# Define the neural network architecture\n","model = nn.Sequential(\n","    # ADD YOUR LAYERS HERE\n",")"]},{"cell_type":"markdown","metadata":{"id":"nn6VDPPw_DIV"},"source":["#### **Step #7: Fit the model**\n","\n","Remember that for:\n","* Regression tasks, we tend to use mse for the loss and rmse for the evaluation metric.\n","* Classification tasks, we tend to use cross entropy for the loss and accuracy for the evaluation metric.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pDWFNUs-_DIV"},"outputs":[],"source":["# Define the loss function\n","\n","# Create the Learner object\n","\n","# Train the model\n"]},{"cell_type":"markdown","metadata":{"id":"LnkSTRRi_DIV"},"source":["#### **Step #8: Evaluate the model**\n","\n","\n","Print the model's rmse on both the training and test sets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WhSAPT28_DIV"},"outputs":[],"source":["# Calculate training rmse\n","train_preds, train_targets = learn.get_preds(dl = train_dl)\n","rmse_score = rmse(train_preds, train_targets)\n","print(\"Training RMSE:\", rmse_score.item())\n","\n","# Calculate validation rmse\n","valid_preds, valid_targets = learn.get_preds(dl = # COMPLETE THIS CODE\n","rmse_score = # COMPLETE THIS CODE\n","print(\"Validation RMSE:\", # COMPLETE THIS CODE"]},{"cell_type":"markdown","metadata":{"id":"87neuXCp_DIV"},"source":["#### **Step #9: Visualize the model's predictions**\n","\n","\n","Now, run the code below to make predictions with the model and output its predictions for items in the validation set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2_j4vqcB_DIV"},"outputs":[],"source":["# Visualize comparison of predictions vs. actual values\n","plt.scatter(valid_targets, valid_preds)\n","plt.plot([valid_targets.min(), valid_targets.max()], [valid_targets.min(), valid_targets.max()], color = 'black', label='Correct Predictions')\n","\n","\n","plt.xlabel('True Value')\n","plt.ylabel('Predicted Value')\n","plt.title('Real vs Value')\n","plt.legend()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"25IYqGfu_DIW"},"source":["<a name=\"p3.2\"></a>\n","\n","---\n","### **Part 3.2: Improvements**\n","---\n","\n","\n","Using the template cell provided below, create and train a neural network that is capable of achieving an 85% or higher accuracy on the test dataset for classifying the people.\n","\n","**Hint:** The first step to improving your work above is adding at least one Hidden Layer. However, you can also consider:\n","* Changing the number of neurons in each Hidden Layer\n","* Changing the activation functions in the Hidden Layers\n","* Changing the activation function in the Output Layer\n","* Training with a different optimizer\n","* Training with a different learning rate\n","* Changing the number of training epochs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"eYRO-y1D_DIW"},"outputs":[],"source":["# BUILD\n","#=======\n","# Define the neural network architecture\n","model = nn.Sequential(\n","    # ADD YOUR LAYERS HERE\n",")\n","\n","\n","\n","# FIT\n","#=====\n","# Define the loss function\n","\n","\n","# Create the Learner object\n","\n","\n","# Train the model\n","\n","\n","\n","\n","# EVALUATE\n","#==========\n","# Calculate training rmse\n","train_preds, train_targets = # COMPLETE THIS CODE\n","rmse_score = rmse(train_preds, train_targets)\n","print(\"Training RMSE:\", rmse_score.item())\n","\n","# Calculate validation rmse\n","valid_preds, valid_targets = # COMPLETE THIS CODE\n","rmse_score = rmse(valid_preds, valid_targets)\n","print(\"Validation RMSE:\", rmse_score.item())"]},{"cell_type":"code","source":["# Visualize comparison of predictions vs. actual values\n","\n","# COMPLETE THIS CODE"],"metadata":{"id":"h9WnQGr7__IH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aQg83H3szxhb"},"source":["<a name=\"p4\"></a>\n","\n","---\n","## **[ADDITIONAL PRACTICE] Part 4: Analysis of the Country211 Dataset**\n","---\n","\n","\n","The Country211 dataset is a [built-in pytorch dataset](https://pytorch.org/vision/main/generated/torchvision.datasets.Country211.html), also available originally from [OpenAI here](https://github.com/openai/CLIP/blob/main/data/country211.md) in which a variety of images are labeled by the country they were taken in.\n","\n","<br>\n","\n","This is largely the same task as you carried out in lab classifying variations of the MNIST dataset, but with the interesting twist that there are *many more* possible labels (unique countries): 211. As such, guessing a label at random would only have a 0.47% chance of being correct (a 0.0047 accuracy) ."]},{"cell_type":"markdown","metadata":{"id":"NsBV-hXjzxhc"},"source":["<a name=\"p4.1\"></a>\n","\n","---\n","### **Part 4.1: The Initial Attempt**\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"UONQfS8nzxhc"},"source":["#### **Step #1: Import and split the dataset into a train/test set**\n","\n","**Run the code below to import, split, and visualize the data.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kvw8-JKmOnsK"},"outputs":[],"source":["from torchvision.datasets import Country211\n","\n","# Define the transformations\n","transform = transforms.Compose([\n","    transforms.Resize((28, 28)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.2860,), (0.3530,))\n","])\n","\n","# Load the dataset\n","train_dataset = Country211(root='./data', split='train', download=True, transform=transform)\n","valid_dataset = Country211(root='./data', split='test', download=True, transform=transform)\n","\n","# Create the DataLoaders object\n","train_dl = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","valid_dl = DataLoader(valid_dataset, batch_size=64)\n","dls = DataLoaders(train_dl, valid_dl)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zzc6PmU5QWLB"},"outputs":[],"source":["# Set the number of images per row and column in the grid\n","n_row = 3\n","n_col = 5\n","\n","# Get a batch of training data\n","images, labels = next(iter(train_dl))\n","\n","# Create a grid of images and labels\n","fig, axs = plt.subplots(n_row, n_col, figsize=(9, 9))\n","for i in range(n_row):\n","    for j in range(n_col):\n","        ax = axs[i, j]\n","        img_idx = i * n_col + j\n","        img = images[img_idx].numpy()\n","        img = np.transpose(img, (1, 2, 0))\n","        label = labels[img_idx].item()\n","        ax.imshow(img, cmap='gray')\n","        ax.set_title(f\"Label: {label}\")\n","        ax.axis('off')\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"lxpEVkplzxhc"},"source":["#### **Step #2: Determine the dimensions of the data**\n","\n","\n","**Determine the number of input neurons and if the data needs to be flattened.**"]},{"cell_type":"code","source":[],"metadata":{"id":"nlIkk93WtsCC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"23qjQva3zxhc"},"source":["#### **Steps #3 - 6: Build the Model**\n","\n","\n","Build a model sequentially such that there is:\n","\n","* The Input Layer, flattened if necessary, and correctly sized for receiving inputs.\n","* No Hidden Layers\n","* The Output Layer with the correct number of neurons given that there are 5749 unique people in the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5_N834XTzxhc"},"outputs":[],"source":["# Define the neural network architecture\n","model = nn.Sequential(\n","    # ADD YOUR LAYERS HERE\n",")"]},{"cell_type":"markdown","metadata":{"id":"wUL8nkIWAdya"},"source":["#### **Step #7: Fit the model**\n","\n","Remember that for:\n","* Regression tasks, we tend to use mse for the loss and rmse for the evaluation metric.\n","* Classification tasks, we tend to use cross entropy for the loss and accuracy for the evaluation metric.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mLU5565azxhc"},"outputs":[],"source":["# Define the loss function\n","\n","# Create the Learner object\n","\n","# Train the model\n"]},{"cell_type":"markdown","metadata":{"id":"LVpS13a5zxhd"},"source":["#### **Step #8: Evaluate the model**\n","\n","\n","Print the model's accuracy on both the training and test sets as percentages."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6iDXuLNSzxhd"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"znWxmzDXzxhd"},"source":["#### **Step #9: Visualize the model's predictions**\n","\n","\n","Now, run the code below to make predictions with the model and output its predictions for items in the validation set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n4jr5bdFzxhd"},"outputs":[],"source":["# Get a batch of validation data\n","val_images, val_labels = next(iter(valid_dl))\n","\n","# Get model predictions for the validation batch\n","val_preds = learn.get_preds(dl=[(val_images, val_labels)])\n","val_pred_labels = torch.argmax(val_preds[0], dim=1)\n","\n","# Set the number of images per row and column in the grid\n","n_row = 3\n","n_col = 5\n","\n","# Create a grid of images and labels with predictions\n","fig, axs = plt.subplots(n_row, n_col, figsize=(9, 9))\n","for i in range(n_row):\n","    for j in range(n_col):\n","        ax = axs[i, j]\n","        img_idx = i * n_col + j\n","        img = val_images[img_idx].numpy()  # Reshape the image to 28x28\n","        img = np.transpose(img, (1, 2, 0))\n","        true_label = val_labels[img_idx].item()\n","        pred_label = val_pred_labels[img_idx].item()\n","        ax.imshow(img, cmap='gray')\n","        ax.set_title(f\"True label: {true_label}\\nPredicted label: {pred_label}\")\n","        ax.axis('off')\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"K1cznrgwzxhd"},"source":["<a name=\"p4.2\"></a>\n","\n","---\n","### **Part 4.2: Improvements**\n","---\n","\n","\n","Using the template cell provided below, create and train a neural network that is better than above. **NOTE**: With some tweaking (and a lot of time for training), we were able to get a little over 0.02 validation accuracy (and 0.05 training accuracy). While seemingly small, 0.02 is a 1/50 chance of being correct versus 1/211 if guessing randomly. However, if you are able to train a model to outperform this--let us know!\n","\n","**Hint:** The first step to improving your work above is adding at least one Hidden Layer. However, you can also consider:\n","* Changing the number of neurons in each Hidden Layer\n","* Changing the activation functions in the Hidden Layers\n","* Changing the activation function in the Output Layer\n","* Training with a different optimizer\n","* Training with a different learning rate\n","* Changing the number of training epochs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"MOGeQW0Uzxhd"},"outputs":[],"source":["# BUILD\n","#=======\n","# Define the neural network architecture\n","model = nn.Sequential(\n","    # ADD YOUR LAYERS HERE\n",")\n","\n","\n","\n","# FIT\n","#=====\n","# Define the loss function\n","\n","\n","# Create the Learner object\n","\n","\n","# Train the model\n","\n","\n","\n","\n","# EVALUATE\n","#==========\n","# Calculate training accuracy\n","train_loss, train_accuracy = # WRITE YOUR CODE HERE\n","print(f\"Training accuracy: {train_accuracy:.4f}\")\n","\n","# Calculate validation accuracy\n","valid_loss, valid_accuracy = # WRITE YOUR CODE HERE\n","print(f\"Validation accuracy: {valid_accuracy:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"Sc5E4-Y6vLbZ"},"source":["# End of notebook\n","\n","---\n","Â© 2024 The Coding School, All rights reserved"]}],"metadata":{"colab":{"provenance":[{"file_id":"1g0r9apHnJDSV9Z8aRq785G5A5IHuL5QK","timestamp":1681344365927},{"file_id":"17XNGWnRqXA7ko59Tp3eGygUEPGGIQ9Ut","timestamp":1659453109668}],"toc_visible":true},"kernelspec":{"display_name":"default:Python","language":"python","name":"conda-env-default-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}