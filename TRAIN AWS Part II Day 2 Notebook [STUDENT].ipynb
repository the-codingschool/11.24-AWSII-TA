{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMFkqzcIp6PMmlm1qCtTG2p"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Lab 2: Classification and Hyperparameter Tuning**\n","---\n","### **Description**\n","This lab provides a comprehensive overview of implementing and evaluating Linear Regression, KNN, and Hyperparameter Tuning.\n","\n","<br>\n","\n","### **Lab Structure**\n","**Part 1**: [Regression with Linear Regression](#p1)\n","\n","**Part 2**: [Classification with KNN](#p2)\n","\n","**Part 3**: [[OPTIONAL] Hyperparameter Tuning](#p3)\n","\n","\n","\n","<br>\n","\n","### **Learning Objectives**\n","By the end of this lab, we will:\n","* Understand how to implement and evaluate Linear Regression and KNN models in sklearn.\n","\n","* Recognize how to perform random and grid searches for hyperparameter tuning.\n","\n","\n","<br>\n","\n","\n","### **Resources**\n","\n","* [Linear Regression with sklearn Cheat Sheet](https://docs.google.com/document/d/1iVieBynTpoKq1LA0kR-4pqDo6evoW5wvbNyE0wOGhYY/edit?usp=drive_link)\n","\n","* [KNN with sklearn Cheat Sheet](https://docs.google.com/document/d/1U-AWXkJEDXZFqhBwFlDjyp9bLsVOeeXGYaxa6SZ7KpY/edit#heading=h.y8q92z25l6we)\n","\n","\n","<br>\n","\n","**Before starting, run the code below to import all necessary functions and libraries.**\n"],"metadata":{"id":"nvymFWiixQvT"}},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","from sklearn import model_selection\n","from sklearn.model_selection import *\n","from sklearn import datasets\n","from sklearn.metrics import *\n","\n","from sklearn.linear_model import LinearRegression\n","from sklearn.neighbors import KNeighborsClassifier"],"metadata":{"id":"b9h5nmwbIYEc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"p1\"></a>\n","\n","---\n","## **Part 1: Regression with Linear Regression**\n","---\n","\n","In this part, we will model the relationship between the numerical features and the `Runtime (min)` variable as the label using linear regression."],"metadata":{"id":"ZcbJjD4lELCc"}},{"cell_type":"markdown","source":["#### **Step #1: Load the data**"],"metadata":{"id":"YQPB7XZ9v-sn"}},{"cell_type":"code","source":["url = \"https://docs.google.com/spreadsheets/d/e/2PACX-1vS9jPkeKJ8QUuAl-fFdg3nJPDP6vx1byvIBl4yW8UZZJ9QEscyALJp1eywKeAg7aAffwdKP63D9osF1/pub?gid=169291584&single=true&output=csv\"\n","movie_df = pd.read_csv(url)\n","\n","movie_df.drop_duplicates(inplace=True)\n","\n","mean_runtime = movie_df['Runtime'].mean()\n","movie_df['Runtime'] = movie_df['Runtime'].fillna(mean_runtime)\n","\n","movie_df = movie_df.rename(columns = {\"Runtime\": \"Runtime (min)\"})\n","movie_df = movie_df.astype({\"Runtime (min)\": \"int64\"})"],"metadata":{"id":"3e2U3qBtVK-1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Step #2: Decide independent and dependent variables**\n","\n","Examining the DataFrame, choose only the numerical variables (other than `Runtime (min)`) for the features and `Runtime (min)` for the label.\n"],"metadata":{"id":"E5z0mLEQF565"}},{"cell_type":"code","source":["features = # COMPLETE THIS CODE\n","label = # COMPLETE THIS CODE"],"metadata":{"id":"Le7RzFSXS7FW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Step #3: Split data into training and testing data**\n","\n","Split the data using a 80 / 20 split."],"metadata":{"id":"lMDn8ONk600c"}},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(# COMPLETE THIS CODE"],"metadata":{"id":"IOuMRPdQlAVz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Step #4: Import your model**"],"metadata":{"id":"5ADG4KRM69dJ"}},{"cell_type":"code","source":["# COMPLETE THIS CODE"],"metadata":{"id":"VZMm4IAClBLO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Step #5: Initialize your model and set hyperparameters**\n","\n","\n","Linear regression takes no hyperparameters, so just initialize the model."],"metadata":{"id":"PFUiA67CHAMc"}},{"cell_type":"code","source":[],"metadata":{"id":"faXqWmQElEFD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Step #6: Fit your model, test on the testing data, and create a visualization if applicable**"],"metadata":{"id":"xMnlj4W0HF9U"}},{"cell_type":"code","source":[],"metadata":{"id":"h5iPDJmJlFAQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### **Create a visualization**\n","\n","Use `y_test` and your `prediction` from the model to create a scatter plot. Then use the following line to visualize where a correct prediction would be. The code has already been given to you.\n","```\n","plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--k', label=\"Correct prediction\")\n","```"],"metadata":{"id":"6DmQtTt9HNJE"}},{"cell_type":"code","source":["plt.figure(figsize=(8, 8))\n","\n","plt.scatter(# COMPLETE THIS CODE\n","plt.plot(# COMPLETE THIS CODE, '--k', label=\"Correct prediction\")\\\n","\n","plt.xlabel(# COMPLETE THIS CODE\n","plt.ylabel(# COMPLETE THIS CODE\n","plt.title(# COMPLETE THIS CODE\n","\n","\n","plt.legend()"],"metadata":{"id":"-kRYw_V5y9qP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Step #7: Evaluate your model**\n","\n","Use mean squared error and the R2 score as the evaluation metrics.\n"],"metadata":{"id":"9hqWuSqc7Tpp"}},{"cell_type":"code","source":["# COMPLETE THIS CODE"],"metadata":{"id":"9Q0ABlESlNDE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# COMPLETE THIS CODE"],"metadata":{"id":"tYyVzoTM3gzM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Step #8: Use the model**\n","\n","Using the model we created, predict the runtime of movies based on the following `Released_Year`, `IMDB_Rating`, `No_of_Votes`, and `Gross`:\n","\n","* `1999`, `7.9`, `100000`, `8000000`\n","* `2007`, `8.5`, `1000000`, `10000000`"],"metadata":{"id":"kKm0SpTVQMkn"}},{"cell_type":"code","source":["movie_df.describe()"],"metadata":{"id":"I6lImblY6OM-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"p2\"></a>\n","\n","---\n","## **Part 2: Classification with KNN**\n","---\n","\n","In this, we will implement a K-Nearest Neighbors (KNN) model aimed at predicting the diagnosis of breast cancer samples. The goal is to classify new samples as either malignant or benign based on their feature characteristics.\n","\n","<br>\n","\n","This dataset contains crucial information related to breast cancer, including various features such as mean radius, mean texture, and mean smoothness. The target variable (label) indicates the diagnosis, distinguishing between malignant and benign cases."],"metadata":{"id":"DrnIqeIqJO16"}},{"cell_type":"markdown","metadata":{"id":"p5tQtCwufJPr"},"source":["#### **Step #1: Load in Data**\n","\n","**Run the code below to load the data.**"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"WtSg7z-3fJPr"},"outputs":[],"source":["from sklearn.datasets import load_breast_cancer\n","data = load_breast_cancer()\n","selected_features = [\"mean radius\", \"mean texture\", \"mean perimeter\", \"mean area\", \"mean smoothness\", \"mean compactness\", \"mean concavity\", \"mean concave points\", \"mean symmetry\", \"mean fractal dimension\"]\n","df = pd.DataFrame(data.data, columns=data.feature_names)\n","df = df[selected_features]\n","df['Target'] = data.target"]},{"cell_type":"markdown","metadata":{"id":"QFGDY2skfJPs"},"source":["#### **Step #2: Choose your Variables**\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k-JjG229fJPs"},"outputs":[],"source":["inputs = # COMPLETE THIS CODE\n","output = # COMPLETE THIS CODE"]},{"cell_type":"markdown","metadata":{"id":"HbEr0TBEfJPs"},"source":["#### **Step #3: Split your Data**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"BiI_7n98fJPs"},"outputs":[],"source":["X_train, X_test, y_train, y_test = # COMPLETE THIS CODE"]},{"cell_type":"markdown","metadata":{"id":"AyBDIqqGfJPs"},"source":["#### **Step #4: Import an ML Algorithm**\n","\n","\n"]},{"cell_type":"code","source":[],"metadata":{"id":"3zUWy-C-fJPs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q4tlM_dQfJPt"},"source":["#### **Step #5: Initialize the Model**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NrqbrvqmfJPt"},"outputs":[],"source":["model = # COMPLETE THIS LINE"]},{"cell_type":"markdown","metadata":{"id":"dwsTR3NBfJPt"},"source":["#### **Step #6: Fit and Test**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nFEKlifHfJPt"},"outputs":[],"source":["model.fit(X_train, # COMPLETE THIS LINE"]},{"cell_type":"code","source":["predictions = # COMPLETE THIS LINE"],"metadata":{"id":"lBki9dH1fJPt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GOjuQCV6fJPu"},"source":["#### **Step #7: Evaluate**\n","\n","Let's evaluate this model and put it to the test! Specifically, use the accuracy score to get a simple overall picture of your model's performance, and the confusion matrix to get a more nuanced view of where the model is performing the best and worst\n"]},{"cell_type":"code","source":["print(accuracy_score(# COMPLETE THIS CODE"],"metadata":{"id":"7iYRgB9yfJPu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cm = confusion_matrix(# COMPLETE THIS CODE\n","disp = ConfusionMatrixDisplay(# COMPLETE THIS CODE\n","disp.plot()\n","plt.show()"],"metadata":{"id":"bF9jaTzyhAjy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BPKZh2EsfJPu"},"source":["#### **Step #8: Apply your Model**\n","\n","You are provided with data from two new breast cancer samples, and you want to assess the predicted class labels (Malignant or Benign) for each of them. The goal is to determine whether either sample is likely to be malignant or benign based on the model's predictions.\n","\n","Here is the data for the two samples:\n","\n","**Sample 1:**\n","\n","* Mean Radius = 12.5\n","* Mean Texture = 18.2\n","* Mean Perimeter = 80.3\n","* Mean Area = 490.2\n","* Mean Smoothness = 0.09\n","* Mean Compactness = 0.08\n","* Mean Concavity = 0.05\n","* Mean Concave Points = 0.03\n","* Mean Symmetry = 0.18\n","* Mean Fractal Dimension = 0.06\n","\n","**Sample 2:**\n","\n","* Mean Radius = 14.3\n","* Mean Texture = 20.8\n","* Mean Perimeter = 92.6\n","* Mean Area = 650.9\n","* Mean Smoothness = 0.1\n","* Mean Compactness = 0.12\n","* Mean Concavity = 0.09\n","* Mean Concave Points = 0.05\n","* Mean Symmetry = 0.2\n","* Mean Fractal Dimension = 0.07\n","\n","You will use your KNN (k-nearest neighbors) model to predict the class labels for these samples and assess their relative likelihood of being malignant or benign based on the predictions."]},{"cell_type":"markdown","metadata":{"id":"fIqwMPFIfJPu"},"source":["##### **1. Predict the diagnosis of Sample 1**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"JwdKPvv7fJPu"},"outputs":[],"source":["sample_1_features = pd.DataFrame([[# COMPLETE THIS CODE\n","\n","prediction_sample_1 = model.predict(# COMPLETE THIS CODE\n","\n","print(\"Predicted label for Sample 1:\", \"Malignant\" if prediction_sample_1[0] == 1 else \"Benign\")"]},{"cell_type":"markdown","metadata":{"id":"3fqNDBVcfJPu"},"source":["##### **2. Predict the diagnosis of Sample 2**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y-WGCnEMfJPv"},"outputs":[],"source":["sample_2_features = pd.DataFrame([[# COMPLETE THIS CODE\n","\n","prediction_sample_2 = model.predict(# COMPLETE THIS CODE\n","\n","print(\"Predicted label for Sample 2:\", \"Malignant\" if prediction_sample_1[0] == 1 else \"Benign\")"]},{"cell_type":"markdown","source":["<a name=\"p3\"></a>\n","\n","---\n","## **[OPTIONAL] Part 3: Hyperparameter Tuning**\n","---\n","\n","In this section, we will explore how to implement random search and grid search for hyperparameter tuning KNN models. Some key points about these approaches:\n","\n","<br>\n","\n","1. **Random Search**\n","\n","This randomly samples hyperparameter combinations from predefined ranges. It's straightforward to implement and can efficiently explore the hyperparameter space without exhaustively searching all possible combinations.\n","\n","It is commonly used in deep learning due to its simplicity and ability to handle large search spaces efficiently. This is particularly important since deep learning models often have many hyperparameters.\n","\n","\n","<br>\n","\n","2. **Grid Search**\n","\n","This exhaustively searches all possible combinations of hyperparameters within predefined ranges. While it's easy to understand and implement, it can be computationally expensive, especially for high-dimensional hyperparameter spaces.\n","\n","This is not commonly used in general due to its inefficiency, unless you have an idea of what range to look at. However, it's valuable to understand a basic way to approach a more structure search than a random one. And in some cases where there are not too many hyperparameters or too wide a range to search, it may actually be useful.\n","\n","\n","<br>\n","\n","**Run the cell below to load and split the data.**"],"metadata":{"id":"MTBjQERGr3di"}},{"cell_type":"code","source":["from sklearn.datasets import load_breast_cancer\n","data = load_breast_cancer()\n","selected_features = [\"mean radius\", \"mean texture\", \"mean perimeter\", \"mean area\", \"mean smoothness\", \"mean compactness\", \"mean concavity\", \"mean concave points\", \"mean symmetry\", \"mean fractal dimension\"]\n","df = pd.DataFrame(data.data, columns=data.feature_names)\n","df = df[selected_features]\n","df['Target'] = data.target\n","\n","inputs = df.drop('Target', axis=1)\n","output = df['Target']\n","\n","X_train, X_test, y_train, y_test = model_selection.train_test_split(inputs, output, test_size = 0.2, random_state = 42)"],"metadata":{"id":"SaRNP63O6nBU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"p3.1\"></a>\n","\n","---\n","### **Part 3.1: Random Search**\n","---\n","\n","In this section, we will see two different ways to run a random search over hyperparameter values. They all accomplish the exact same thing and there is no reason to believe that one will produce better models in the end. However, each approach does have its pros/cons in terms of ease of use, efficiency, etc.\n","\n","<br>\n","\n","**NOTE**: We are using relatively conventional hyperparameter values to search within as follows:\n","* Using `10**np.random.uniform(-4, -2)` generates learning rates in the range $[0.0001, 0.01]$.\n","* This range is chosen because it covers small to moderately large learning rates, which are commonly effective for training neural networks.\n","* The exponential scale ensures that we can effectively explore a wide range of learning rates in our random search."],"metadata":{"id":"syn5Ehg3uthK"}},{"cell_type":"markdown","source":["#### **Problem #3.1.1**\n","\n","Purely for the sake of example, here is how we can perform a random search manually.\n","\n","\n","\n","<br>\n","\n","**NOTES**:\n","* Simplicity: Straightforward to implement; you manually define a loop to try different hyperparameter combinations.\n","* Flexibility: High flexibility since you control the entire process, but it requires more code.\n","* Visualization: Manual collection and plotting of results, giving you full control over how to visualize the performance.\n","* Real-world Usage: Less common in large-scale applications due to the manual effort required."],"metadata":{"id":"1Z1Vcq0Zr9ZP"}},{"cell_type":"code","source":["max_points = len(X_train)\n","\n","\n","results = []\n","for _ in range(10):  # Number of random samples\n","    n_neighbors = np.random.randint(1, max_points)\n","    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n","    knn.fit(X_train, y_train)\n","    accuracy = accuracy_score(y_test, knn.predict(X_test))\n","    results.append({'n_neighbors': n_neighbors, 'accuracy': accuracy})\n","\n","# Convert results to a DataFrame\n","df_results_knn_manual = pd.DataFrame(results)\n","\n","# Plot results\n","plt.figure(figsize=(10, 6))\n","plt.scatter(df_results_knn_manual['n_neighbors'], df_results_knn_manual['accuracy'], cmap='viridis')\n","plt.xlabel('Number of Neighbors')\n","plt.ylabel('Accuracy')\n","plt.title('Manual Random Search Results for KNN')\n","plt.show()"],"metadata":{"id":"5DNhWNpJ15Vl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #3.1.2**\n","\n","Here, we use `RandomizedSearchCV` from `sklearn` that performs a random search using cross validation.\n","\n","<br>\n","\n","**NOTES**:\n","* Simplicity: More concise and automated than manual search. It leverages RandomizedSearchCV from sklearn for hyperparameter tuning.\n","* Flexibility: Allows you to define a range of hyperparameters to sample from. Automated cross-validation is built-in.\n","* Visualization: Automated collection of results, making it easy to analyze and visualize performance metrics.\n","* Real-world Usage: Widely used in practice for its automation and integration with scikit-learn's ecosystem."],"metadata":{"id":"muwSNm4PwOHt"}},{"cell_type":"code","source":["max_points = len(X_train)\n","\n","# Define the model\n","knn = KNeighborsClassifier()\n","\n","# Define the hyperparameter grid\n","param_dist = {\n","    'n_neighbors': np.arange(1, max_points)\n","}\n","\n","# Use RandomizedSearchCV\n","random_search_knn = RandomizedSearchCV(estimator=knn, param_distributions=param_dist, n_iter=10, cv=3, verbose=2)\n","random_search_knn_result = random_search_knn.fit(X_train, y_train)\n","\n","# Convert results to DataFrame\n","df_results_knn_cv = pd.DataFrame(random_search_knn_result.cv_results_)\n","\n","# Plot results\n","plt.figure(figsize=(10, 6))\n","plt.scatter(df_results_knn_cv['param_n_neighbors'], df_results_knn_cv['mean_test_score'], cmap='viridis')\n","\n","plt.xlabel('Number of Neighbors')\n","plt.ylabel('Mean Test Score')\n","plt.title('RandomizedSearchCV Results for KNN')\n","plt.show()"],"metadata":{"id":"SQUM3zF52QWr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #3.1.3**\n","\n","Now, modify the code from above to test 20 random values of K instead of 10."],"metadata":{"id":"w2PKTfws6Pwi"}},{"cell_type":"code","source":["# COMPLETE THIS CODE"],"metadata":{"id":"iYLkaLsP86yq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #3.1.4**\n","\n","Next, modify the code from above (with 20 K values) but only for *odd* K values to avoid ties."],"metadata":{"id":"mpyZEHaY8f2w"}},{"cell_type":"code","source":["# COMPLETE THIS CODE"],"metadata":{"id":"5Qn72Kf58-Qd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #3.1.5**\n","\n","Now, train a single KNN model with the best K value seen in the last problem and look at its confusion matrix on the test set.\n","\n","<br>\n","\n","**NOTE**: There are several ways to do this, but one of the easiest is to use the `.best_params_` attribute of the `RandomizedSearchCV` object after fitting."],"metadata":{"id":"20ULXfk59AIb"}},{"cell_type":"code","source":["k_value = random_search_knn_result.best_params_['n_neighbors']\n","\n","print(k_value)"],"metadata":{"id":"hOcf5klpAu-I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = # COMPLETE THIS CODE\n","model.# COMPLETE THIS CODE\n","\n","predictions = # COMPLETE THIS CODE\n","\n","cm = confusion_matrix(y_test, predictions, labels=model.classes_)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n","disp.plot()\n","plt.show()"],"metadata":{"id":"Mx47_gsc9cV6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"p3.2\"></a>\n","\n","---\n","### **Part 3.2: Grid Search**\n","---\n","\n","In this section, we will see several different ways to run a grid search over hyperparameter values. They all accomplish the exact same thing and there is no reason to believe that one will produce better models in the end. However, each approach does have its pros/cons in terms of ease of use, efficiency, etc."],"metadata":{"id":"i0Qv8DG326qZ"}},{"cell_type":"markdown","source":["#### **Problem #3.2.1**\n","\n","Purely for the sake of example, here is how we can perform a grid search manually.\n","\n","\n","\n","<br>\n","\n","**NOTES**:\n","* Simplicity: Straightforward to implement; you manually define a loop to try different hyperparameter combinations.\n","* Flexibility: High flexibility since you control the entire process, but it requires more code.\n","* Visualization: Manual collection and plotting of results, giving you full control over how to visualize the performance.\n","* Real-world Usage: Less common in large-scale applications due to the manual effort required."],"metadata":{"id":"hgQzSBHV26qo"}},{"cell_type":"code","source":["max_points = len(X_train)\n","\n","\n","# Grid Search\n","results = []\n","k_grid = np.linspace(1, max_points, num=10)\n","\n","for i in range(10):\n","    n_neighbors = int(k_grid[i])\n","    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n","    knn.fit(X_train, y_train)\n","    accuracy = accuracy_score(y_test, knn.predict(X_test))\n","    results.append({'n_neighbors': n_neighbors, 'accuracy': accuracy})\n","\n","# Convert results to a DataFrame\n","df_results_knn_manual = pd.DataFrame(results)\n","\n","# Plot results\n","plt.figure(figsize=(10, 6))\n","plt.scatter(df_results_knn_manual['n_neighbors'], df_results_knn_manual['accuracy'])\n","\n","plt.xlabel('Number of Neighbors')\n","plt.ylabel('Accuracy')\n","plt.title('Manual Grid Search Results for KNN')\n","plt.show()"],"metadata":{"id":"zXwMJMVv26qp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #3.2.2**\n","\n","Here, we use `GridSearchCV` from `sklearn` that performs a grid search using cross validation. In order to use `sklearn` methods with neural networks built in `keras`, we must install and use a \"wrapper\" such as `KerasClassifer` first.\n","\n","<br>\n","\n","**NOTES**:\n","* Simplicity: More concise and automated than manual search. It leverages GridSearchCV from sklearn for hyperparameter tuning.\n","* Flexibility: Allows you to define a range of hyperparameters to sample from. Automated cross-validation is built-in.\n","* Visualization: Automated collection of results, making it easy to analyze and visualize performance metrics.\n","* Real-world Usage: Widely used in practice for its automation and integration with scikit-learn's ecosystem."],"metadata":{"id":"5CMvNmw726qq"}},{"cell_type":"code","source":["max_points = len(X_train)\n","\n","# Define the KNN model\n","knn = KNeighborsClassifier()\n","\n","# Define the hyperparameter grid\n","param_grid_knn = {\n","    'n_neighbors': np.arange(1, max_points, 50),\n","}\n","\n","# Use GridSearchCV\n","grid_search_knn = GridSearchCV(estimator=knn, param_grid=param_grid_knn, cv=3, verbose=2)\n","grid_search_knn_result = grid_search_knn.fit(X_train, y_train)\n","\n","# Convert results to DataFrame\n","df_results_knn_grid = pd.DataFrame(grid_search_knn_result.cv_results_)\n","\n","# Plot results\n","plt.figure(figsize=(10, 6))\n","plt.scatter(df_results_knn_grid['param_n_neighbors'], df_results_knn_grid['mean_test_score'])\n","\n","plt.xlabel('Number of Neighbors')\n","plt.ylabel('Mean Test Score')\n","plt.title('Grid Search Results for KNN')\n","plt.show()"],"metadata":{"id":"Utob8IwD26qq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #3.2.3**\n","\n","Now, alter this so that the grid of K values is in steps of 10 instead of 50. This is a finer grid, which takes longer to search but is more exhuastive."],"metadata":{"id":"MH9i7Jv---IO"}},{"cell_type":"code","source":["# COMPLETE THIS CODE"],"metadata":{"id":"tW-vp1D6_KmQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #3.2.4**\n","\n","From the problem above, determine the range of 50 K values that seem to have the highest performance (you can do this by eye or by inspecting the resulting scores themselves). Then, in the space below perform a grid search on every single one of these 50 values."],"metadata":{"id":"-WPNTKQb_ZzM"}},{"cell_type":"code","source":["# COMPLETE THIS CODE"],"metadata":{"id":"SmBcCgCpAMyP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #3.2.5**\n","\n","Now, train a KNN model with the best K value from the problem above and look at its confusion matrix."],"metadata":{"id":"Np0XV63JAXWk"}},{"cell_type":"code","source":["# COMPLETE THIS CODE"],"metadata":{"id":"-07GHnYxAgBx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","# End of Notebook\n","\n","© 2024 The Coding School, All rights reserved"],"metadata":{"id":"75g0yrIewirj"}}]}