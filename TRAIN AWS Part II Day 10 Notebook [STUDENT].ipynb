{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyOYisrQLOTmYtVjPr0Lgc7Q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Day 10: Optimizing Models**\n","---\n","\n","### **Description**\n","This notebook shows how to implement and use the topics that we've been discussing in optimizing models.\n","\n","<br>\n","\n","### **Structure**\n","**Part 1**: [Hyperparameter Tuning](#p1)\n",">\n","> **Part 1.1**: [Random Search](#p1.1)\n",">\n","> **Part 1.2**: [Bayesian Optimization](#p1.2)\n","\n","**Part 2**: [Early Stopping and Regularization](#p2)\n","\n","**Part 3**: [Challenge Problem](#p3)\n","\n","<br>\n","\n","### **Learning Objectives**\n","\n","1. Recognize and implement various hyperparameter tuning techniques, including random search and Bayesian optimization, to optimize the performance of neural network models using pytorch and hyperopt.\n","\n","1. Recognize the concepts and applications of early stopping and regularization techniques (L2 and dropout) in preventing overfitting and improving the generalization of machine learning models.\n","\n","\n","<br>\n","\n","### **Resources**\n","* [Model Optimization Cheat Sheet](https://docs.google.com/document/d/1O3_U4EAp1QsFSq894d0B6mZ-CiAvFhU7s0vpn9iyuSw/edit)\n","\n","<br>\n","\n","**Run the cell below to load all necessary functions and libraries. NOTE: You may have to restart the notebook after running the first cell of installations.**"],"metadata":{"id":"mbZXQ3rA3NwL"}},{"cell_type":"code","source":["!pip --quiet install scikit-learn scikit-optimize\n","!pip --quiet install torchview torch graphviz\n","!pip --quiet install fastai\n","!pip --quiet install hyperopt\n","!conda install -q python-graphviz"],"metadata":{"id":"KycLMEzJ9bu-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import classification_report, accuracy_score\n","\n","\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.datasets import make_classification, load_breast_cancer\n","\n","import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","from torchvision.datasets import MNIST\n","\n","from fastai.vision.all import *\n","from fastai.tabular.all import *\n","\n","from torchview import draw_graph\n","\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","from hyperopt import fmin, tpe, rand, hp, Trials, STATUS_OK\n","\n","\n","\n","def binary_accuracy(y_pred, y_true):\n","    # Output 0 if y_pred <= 0.5 and 1 if y_pred is > 0.5\n","    y_pred = (y_pred > 0.5).float()\n","    # Returns accuracy\n","    return (y_pred == y_true).float().mean()\n","\n","\n","\n","# Generate some synthetic data\n","X, y = make_classification(n_samples = 1000, n_features = 20, n_informative = 10, class_sep=0.01, random_state=28)\n","\n","\n","# Split the data\n","X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize the data\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_valid = scaler.transform(X_valid)\n","\n","\n","# Convert the numpy arrays to PyTorch tensors with float32 data type\n","X_train_torch = torch.tensor(X_train, dtype=torch.float32)\n","y_train_torch = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n","\n","X_valid_torch = torch.tensor(X_valid, dtype=torch.float32)\n","y_valid_torch = torch.tensor(y_valid, dtype=torch.float32).unsqueeze(1)\n","\n","# Create dataset object\n","train_ds = list(zip(X_train_torch, y_train_torch))\n","valid_ds = list(zip(X_valid_torch, y_valid_torch))\n","\n","# Define the DataLoaders\n","train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n","valid_dl = DataLoader(valid_ds, batch_size=64, shuffle=True)\n","\n","dls = DataLoaders(train_dl, valid_dl)"],"metadata":{"id":"Hr64NBbbrya0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"p1\"></a>\n","\n","---\n","## **Part 1: Hyperparameter Tuning**\n","---\n","\n","In this section, we will explore how to implement random search, grid search, and Bayesian optimization for hyperparameter tuning with neural networks. This will build upon what we've seen already for KNN. Some key points about these approaches:\n","\n","<br>\n","\n","1. **Random Search**\n","\n","This randomly samples hyperparameter combinations from predefined ranges. It's straightforward to implement and can efficiently explore the hyperparameter space without exhaustively searching all possible combinations.\n","\n","It is commonly used in deep learning due to its simplicity and ability to handle large search spaces efficiently. This is particularly important since deep learning models often have many hyperparameters.\n","\n","\n","<br>\n","\n","2. **Grid Search**\n","\n","This exhaustively searches all possible combinations of hyperparameters within predefined ranges. While it's easy to understand and implement, it can be computationally expensive, especially for high-dimensional hyperparameter spaces.\n","\n","This is not commonly used in general due to its inefficiency. However, it's valuable to understand a basic way to approach a more structure search than a random one. And in some cases where there are not too many hyperparameters or too wide a range to search, it may actually be useful.\n","\n","\n","<br>\n","\n","3. **Bayesian Optimization**\n","\n","This uses probabilistic models to guide the search for optimal hyperparameters. It's more computationally efficient than grid search and can adaptively explore promising regions of the search space, leading to faster convergence.\n","\n","It is increasingly popular in deep learning due to its efficiency and ability to handle complex search spaces. It's particularly well-suited for deep learning models with many hyperparameters, as it can efficiently explore the space and find optimal configurations."],"metadata":{"id":"MTBjQERGr3di"}},{"cell_type":"markdown","source":["<a name=\"p1.1\"></a>\n","\n","---\n","### **Part 1.1: Random Search**\n","---\n","\n","In this section, we will see how to run a random search over hyperparameter values.\n","\n","<br>\n","\n","**NOTE**: We are using relatively conventional hyperparameter values to search within as follows:\n","* Using `10**np.random.uniform(-4, -2)` generates learning rates in the range $[0.0001, 0.01]$.\n","* This range is chosen because it covers small to moderately large learning rates, which are commonly effective for training neural networks.\n","* The exponential scale ensures that we can effectively explore a wide range of learning rates in our random search."],"metadata":{"id":"syn5Ehg3uthK"}},{"cell_type":"markdown","source":["#### **Problem #1.1.1**\n","\n"],"metadata":{"id":"YphSUlW7wWmM"}},{"cell_type":"code","source":["from hyperopt import fmin, tpe, hp, Trials\n","\n","# Define the objective function to minimize (Hyperopt minimizes the loss, so we negate accuracy)\n","def objective(params):\n","\n","    model = nn.Sequential(\n","      nn.Linear(20, params['neurons']),\n","      nn.ReLU(),\n","      nn.Linear(params['neurons'], 1),\n","      nn.Sigmoid()\n","    )\n","\n","    loss_func = nn.BCELoss()\n","    learn = Learner(dls, model, loss_func=loss_func, metrics=binary_accuracy)\n","    learn.fit(10, lr=params['learning_rate'])\n","\n","    valid_loss, valid_accuracy = learn.validate()\n","\n","    return {'loss': -valid_accuracy, 'status': STATUS_OK}\n","\n","\n","\n","# Define the search space\n","space = {\n","    'neurons': hp.randint('neurons', 1, 5),\n","    'learning_rate': hp.loguniform('learning_rate', np.log(1e-2), np.log(1e-1))\n","}\n","\n","# Perform optimization\n","best = fmin(fn=objective, space=space, algo=rand.suggest, max_evals=5)\n","print(best)"],"metadata":{"id":"LAhIZsMM1xg3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #1.1.2**\n","\n","Although the setup can look complicated, we can rerun this search for a different range of hyperparameter values with about half of the code. For example, the code below searches the following range:\n","* Neurons: 10 to 50\n","* Learning rate: 1e-3 to 1e-1\n","* Maximum number of evaluations: 10"],"metadata":{"id":"ukg80xiIjVNw"}},{"cell_type":"code","source":["# Define the search space\n","space = {\n","    'neurons': hp.randint('neurons', 10, 50),\n","    'learning_rate': hp.loguniform('learning_rate', np.log(1e-3), np.log(1e-1))\n","}\n","\n","# Perform optimization\n","best = fmin(fn=objective, space=space, algo=rand.suggest, max_evals=10)\n","print(best)"],"metadata":{"id":"23k2o5_ZjU81"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #1.1.3**\n","\n","Now, modify the code above to perform a random search of 15 different values within these hyperparameter ranges:\n","* Number of neurons: 1 to 150\n","* Learning rate: 1e-5 to 1e-3\n","* Maximum number of evaluations: 10"],"metadata":{"id":"GY08M02E8Vrx"}},{"cell_type":"code","source":["# COMPLETE THIS CODE"],"metadata":{"id":"t5hVlaK982E4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"p1.2\"></a>\n","\n","---\n","### **Part 1.2: Bayesian Optimization**\n","---\n","\n","In this section, we will see several different ways to run a Bayesian optimization over hyperparameter values. They all accomplish the exact same thing and there is no reason to believe that one will produce better models in the end. However, each approach does have its pros/cons in terms of ease of use, efficiency, etc.\n","\n","<br>\n","\n","\n","**NOTE**: Due to the deeper complexity of this approach, we will not be manually implementing it."],"metadata":{"id":"LaRDLlui6hKA"}},{"cell_type":"markdown","source":["#### **Problem #1.2.1**\n","\n","Now, let's use hyperopt again, but with `algo=tpe.suggest` instead of `algo=rand.suggest` to use Bayesian optimization instead of random search."],"metadata":{"id":"UbPglAhx6hKD"}},{"cell_type":"code","source":["from hyperopt import fmin, tpe, hp, Trials\n","\n","# Define the objective function to minimize (Hyperopt minimizes the loss, so we negate accuracy)\n","def objective(params):\n","\n","    model = nn.Sequential(\n","      nn.Linear(20, params['neurons']),\n","      nn.ReLU(),\n","      nn.Linear(params['neurons'], 1),\n","      nn.Sigmoid()\n","    )\n","\n","    loss_func = nn.BCELoss()\n","    learn = Learner(dls, model, loss_func=loss_func, metrics=binary_accuracy)\n","    learn.fit(10, lr=params['learning_rate'])\n","\n","    valid_loss, valid_accuracy = learn.validate()\n","\n","    return {'loss': -valid_accuracy, 'status': STATUS_OK}\n","\n","\n","\n","# Define the search space\n","space = {\n","    'neurons': hp.randint('neurons', 1, 5),\n","    'learning_rate': hp.loguniform('learning_rate', np.log(1e-2), np.log(1e-1))\n","}\n","\n","# Perform optimization\n","best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=5)\n","print(best)"],"metadata":{"id":"9t_S8Z0Izw5z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #1.2.2**\n","\n","Now perform Bayesian Optimization for the hyperparameter range:\n","* Number of neurons: 1 to 150\n","* Learning rate: 1e-5 to 1e-3\n","* Maximum number of evaluations: 10"],"metadata":{"id":"QAJHBD-pE-ee"}},{"cell_type":"code","source":["# COMPLETE THIS CODE"],"metadata":{"id":"hl9ltC1wioIZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #1.2.3**\n","\n","Perform Bayesian Optimization for the hyperparameter range:\n","* Number of neurons: 50 to 150\n","* Learning rate: 1e-5 to 1e-1\n","* Max evaluations: 20"],"metadata":{"id":"tifvcCEQFSiw"}},{"cell_type":"code","source":["# COMPLETE THIS CODE"],"metadata":{"id":"P8EsqeptF9mP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #1.2.4**\n","\n","Since we did not see Bayesian Optimization for KNN last week, we will also show you how to use `BayesSearchCV` from `skopt` (short for `scikit-optimize`). **NOTE**: This only works with `sklearn` models."],"metadata":{"id":"qKr1cvKe6hKD"}},{"cell_type":"code","source":["from skopt import BayesSearchCV\n","from skopt.space import Integer\n","\n","# A strangely necessary fix for using BayesSearchCV that others have raised, but has not been addressed yet :/\n","np.int = int\n","\n","\n","# Define the KNN model\n","knn = KNeighborsClassifier()\n","\n","# Define the hyperparameter search space\n","search_spaces_knn = {\n","    'n_neighbors': Integer(1, 50)\n","}\n","\n","# Use BayesSearchCV\n","bayes_search_knn = BayesSearchCV(estimator=knn, search_spaces=search_spaces_knn, n_iter=10, cv=3, verbose=2)\n","bayes_search_knn_result = bayes_search_knn.fit(X_train, y_train)\n","\n","\n","\n","# Convert results to DataFrame\n","df_results_bayes_knn = pd.DataFrame(bayes_search_knn_result.cv_results_)\n","\n","# Plot results\n","plt.figure(figsize=(10, 6))\n","plt.scatter(df_results_bayes_knn['param_n_neighbors'], df_results_bayes_knn['mean_test_score'])\n","\n","plt.xlabel('Number of Neighbors')\n","plt.ylabel('Mean Test Score')\n","plt.title('Bayesian Optimization Results for KNN')\n","plt.show()"],"metadata":{"id":"g8thliCM6hKD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"p2\"></a>\n","\n","---\n","## **Part 2: Early Stopping and Regularization**\n","---\n","\n","In this section, we explore different regularization techniques. Each regularization technique has its own advantages and use cases, and they can also be combined for even better regularization performance, such as using L1 and L2 regularization together (ElasticNet) or combining dropout with either L1 or L2 regularization in neural networks. Here is an overview of the main types we'll explore:\n","\n","<br>\n","\n","**No Regularization**\n","\n","No additional penalty is added to the loss function.\n","May lead to overfitting, especially when dealing with complex models or limited training data.\n","\n","<br>\n","\n","**L1 Regularization**\n","\n","Adds a penalty to the loss function proportional to the absolute value of the weights. Encourages sparsity in the weights, leading to some weights being set to exactly zero. Useful for feature selection, as it can effectively prune less important features by setting their corresponding weights to zero. May result in a more interpretable model with fewer parameters.\n","\n","<br>\n","\n","**L2 Regularization**\n","\n","Adds a penalty to the loss function proportional to the square of the weights. Discourages large weights, preventing individual weights from becoming too large and dominating the learning process. Helps to control overfitting by smoothing out the learning landscape and reducing the model's sensitivity to small changes in input features. Does not lead to sparsity in the weights.\n","\n","<br>\n","\n","\n","**Dropout**\n","\n","Randomly sets a fraction of input units to zero during training. Mimics an ensemble of multiple models by dropping out different neurons during each training iteration, effectively preventing co-adaptation of neurons. Helps to prevent overfitting by adding noise to the network and promoting the learning of more robust features.\n","Does not involve adding any regularization penalty to the loss function.\n","\n","<br>\n","\n","**Run the code below to the load the dataset that we will use in this section.**"],"metadata":{"id":"KDyR6zQcL6PA"}},{"cell_type":"code","source":["# Load dataset\n","url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n","names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n","data = pd.read_csv(url, names=names)\n","\n","# Split data into features and target\n","X = data.drop('class', axis=1)\n","y = np.array(data['class'])\n","\n","# Split data into training and testing sets\n","X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_valid_scaled = scaler.transform(X_valid)\n","\n","\n","# Convert the numpy arrays to PyTorch tensors with float32 data type\n","X_train_torch = torch.tensor(X_train_scaled, dtype=torch.float32)\n","y_train_torch = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n","\n","X_valid_torch = torch.tensor(X_valid_scaled, dtype=torch.float32)\n","y_valid_torch = torch.tensor(y_valid, dtype=torch.float32).unsqueeze(1)\n","\n","# Create dataset object\n","train_ds = list(zip(X_train_torch, y_train_torch))\n","valid_ds = list(zip(X_valid_torch, y_valid_torch))\n","\n","# Define the DataLoaders\n","train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n","valid_dl = DataLoader(valid_ds, batch_size=64, shuffle=True)\n","\n","dls = DataLoaders(train_dl, valid_dl)"],"metadata":{"id":"7s4Q9UqvNhls"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #2.1**\n","\n","First, let's train a baseline model to compare to, meaning that this does not use any regularization techniques."],"metadata":{"id":"wPpND5dnNU1P"}},{"cell_type":"code","source":["model = nn.Sequential(\n","  nn.Linear(8, 12),\n","  nn.ReLU(),\n","  nn.Linear(12, 8),\n","  nn.ReLU(),\n","  nn.Linear(8, 1),\n","  nn.Sigmoid()\n",")\n","\n","# Training\n","loss_func = nn.BCELoss()\n","learn_baseline = Learner(dls, model, loss_func=loss_func, metrics=binary_accuracy)\n","learn_baseline.fit(100, lr=0.001)\n","\n","\n","# Plot the training and validation losses\n","learn_baseline.recorder.plot_loss()\n","plt.show()"],"metadata":{"id":"2Y3Lhe9-m64O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #2.2**\n","\n","Now, let's explore the use of early stopping."],"metadata":{"id":"6jHRDbq-N9Eo"}},{"cell_type":"code","source":["from fastai.callback.tracker import EarlyStoppingCallback\n","\n","\n","model = nn.Sequential(\n","  nn.Linear(8, 12),\n","  nn.ReLU(),\n","  nn.Linear(12, 8),\n","  nn.ReLU(),\n","  nn.Linear(8, 1),\n","  nn.Sigmoid()\n",")\n","\n","# Training\n","es_cb = EarlyStoppingCallback(monitor='valid_loss', min_delta=0.01, patience=3)\n","\n","loss_func = nn.BCELoss()\n","learn_es = Learner(dls, model, loss_func=loss_func, metrics=binary_accuracy, cbs = [es_cb])\n","learn_es.fit(100, lr=0.001)\n","\n","\n","# Plot the training and validation losses\n","learn_es.recorder.plot_loss()\n","plt.show()"],"metadata":{"id":"wIQXazjrOBf3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #2.3**\n","\n","Here, we use L2 regularization."],"metadata":{"id":"7vupw6p5OWcb"}},{"cell_type":"code","source":["model = nn.Sequential(\n","  nn.Linear(8, 12),\n","  nn.ReLU(),\n","  nn.Linear(12, 8),\n","  nn.ReLU(),\n","  nn.Linear(8, 1),\n","  nn.Sigmoid()\n",")\n","\n","# Training\n","loss_func = nn.BCELoss()\n","learn_l2 = Learner(dls, model, loss_func=loss_func, metrics=binary_accuracy, wd = 1e-6)\n","learn_l2.fit(100, lr=0.001)\n","\n","\n","# Plot the training and validation losses\n","learn_l2.recorder.plot_loss()\n","plt.show()"],"metadata":{"id":"0TYhWOCrOkAP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #2.4**\n","\n","Lastly, we look at the results of using a dropout regularization layer."],"metadata":{"id":"ddD0MsjJPt6k"}},{"cell_type":"code","source":["model = nn.Sequential(\n","  nn.Linear(8, 12),\n","  nn.ReLU(),\n","  nn.Dropout(p=0.4),\n","  nn.Linear(12, 8),\n","  nn.ReLU(),\n","  nn.Linear(8, 1),\n","  nn.Sigmoid()\n",")\n","\n","# Training\n","loss_func = nn.BCELoss()\n","learn_drop = Learner(dls, model, loss_func=loss_func, metrics=binary_accuracy)\n","learn_drop.fit(100, lr=0.001)\n","\n","\n","# Plot the training and validation losses\n","learn_drop.recorder.plot_loss()\n","plt.show()"],"metadata":{"id":"73kZReT5P1WK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #2.5**\n","\n","It's easiest to compare the models by looking at their learning curves all together."],"metadata":{"id":"uAFGiGBoQWSm"}},{"cell_type":"code","source":["metric_idx = 2\n","baseline_accuracy = [values[metric_idx] for values in learn_baseline.recorder.values]\n","es_accuracy = [values[metric_idx] for values in learn_es.recorder.values]\n","l2_accuracy = [values[metric_idx] for values in learn_l2.recorder.values]\n","drop_accuracy = [values[metric_idx] for values in learn_drop.recorder.values]\n","\n","\n","# Plot learning curves for all neural networks\n","plt.figure(figsize=(10, 6))\n","\n","plt.plot(baseline_accuracy, label='No Regularization (Baseline)')\n","plt.plot(es_accuracy, label='Early Stopping')\n","plt.plot(l2_accuracy, label='L2 Regularization')\n","plt.plot(drop_accuracy, label='Dropout')\n","\n","\n","# Plot labels and legend\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.title('Learning Curves of Neural Networks')\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"jevIAYAuQRO0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #2.6**\n","\n","Complete the code below to apply all three approaches at the same time."],"metadata":{"id":"7jEVS7VKud3K"}},{"cell_type":"code","source":["# COMPLETE THIS CODE"],"metadata":{"id":"rcc2PfdQunj8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"p3\"></a>\n","\n","---\n","## **Part 3: Challenge Problem**\n","---\n","\n","In this section, it's up to you to apply what you've learned above to accomplish the following open ended challenge: build and compare different machine learning models for classifying fashion items from images using the MNIST dataset. You will explore neural network models with various regularization techniques, early stopping, L2 regularization, and dropout. Your goal is to determine which model performs best in terms of accuracy. Additionally, you will perform hyperparameter tuning using random search or Bayesian optimization to find the best hyperparameters for your models.\n","\n","<br>\n","\n","**NOTES**:\n","* The data is images of handwritten digits that are to be classified. However, they have been turned from a 28x28 grid (matrix) of pixels into a single 784 long column (vector) of pixel values.\n","* Testing all possible models is simply not feasible! It will take far too long without more advanced equipment and approaches like multiprocessing. Focus on just a few techniques and models here to keep your total training time within a reasonable limit (**15 - 20 minutes** should be enough time to perform about 5 searches. It took about 23 minutes for us to acheive a nearly 0.977 accuracy, which is considered pretty strong performance on the MNIST dataset.\n","* To search for a good value of dropoout, you can use `hp.uniform('name', 0, 1)` to search for values between 0 and 1."],"metadata":{"id":"ZhKy8w4YRW5b"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.preprocessing import StandardScaler\n","\n","from torchvision.datasets import MNIST\n","\n","from hyperopt import hp, tpe, fmin, STATUS_OK, Trials\n","import time\n","\n","\n","# Define the transformations\n","transform = transforms.Compose([\n","    transforms.Resize((28, 28)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.1307,), (0.3081,))\n","])\n","\n","# Load the MNIST dataset\n","train_dataset = MNIST(root='./data',\n","                      train=True,\n","                      download=True,\n","                      transform=transform)\n","valid_dataset = MNIST(root='./data',\n","                      train=False,\n","                      download=True,\n","                      transform=transform)\n","\n","# Create the DataLoaders object\n","train_dl = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","valid_dl = DataLoader(valid_dataset, batch_size=64, shuffle=True)\n","dls = DataLoaders(train_dl, valid_dl)\n","\n","# Set the number of images per row and column in the grid\n","n_row = 4\n","n_col = 4\n","\n","# Get a batch of training data\n","images, labels = next(iter(train_dl))\n","\n","# Create a grid of images and labels\n","fig, axs = plt.subplots(n_row, n_col, figsize=(9, 9))\n","for i in range(n_row):\n","    for j in range(n_col):\n","        ax = axs[i, j]\n","        img_idx = i * n_col + j\n","        img = images[img_idx].reshape(28, 28).numpy()  # Reshape the image to 28x28\n","        label = labels[img_idx].item()\n","        ax.imshow(img, cmap='gray')\n","        ax.set_title(f\"Label: {label}\")\n","        ax.axis('off')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"sltAknoAMSBF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# COMPLETE THIS CODE"],"metadata":{"id":"9LUhtB3QNdcV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#End of notebook\n","---\n","© 2024 The Coding School, All rights reserved"],"metadata":{"id":"7dzC09dLlEhm"}}]}