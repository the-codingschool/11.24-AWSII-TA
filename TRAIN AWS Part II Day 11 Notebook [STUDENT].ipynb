{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"10bfYS1dC3tauAlicux4iEW2hJc1NVv8-","timestamp":1682709808259}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **Lab 11: Final Projects**\n","---\n","\n","### **Description**\n","In today's notebook, you will apply what you have learned throughout Part II, particularly with regards to Deep Learning and Natural Language Processsing, to several projects in order to reinforce these skills. We encourage you to not only solve the problems and follow the steps below, but to also consider how you could apply what else you've learned to each problem or in coming up with new problems to solve with the given datasets.\n","\n","<br>\n","\n","### **Lab Structure**\n","**Part 1**: [Heart Attack Predictor](#p1)\n","\n","**Part 2**: [Amazon Review Sentiment Analysis](#p2)\n","\n","**Part 3**: [Semantic Segmentation with U-Net](#p3)\n","\n","**Part 4**: [Generating Wikipedia Entries](#p4)\n","\n","\n","<br>\n","\n","### **Goals**\n","By the end of this lab, you will have honed the skills you have learned throughout the program and started to see how you could extend them to more complex situations.\n","\n","<br>\n","\n","### **Cheat Sheets**\n","* [EDA with pandas](https://docs.google.com/document/d/1FFoqw45P-kuoq912ARP4qfdGeLTqoq73_qjZThPp2_8/edit?usp=drive_link)\n","\n","* [Data Visualization with matplotlib](https://docs.google.com/document/d/1YlUp6ll81qOyDpU1OWzE-SPxQ3hnF5C9ukLRL_6PYKE/edit?usp=drive_link)\n","\n","* [Linear Regression with sklearn](https://docs.google.com/document/d/1iVieBynTpoKq1LA0kR-4pqDo6evoW5wvbNyE0wOGhYY/edit?usp=drive_link)\n","\n","* [KNN with sklearn](https://docs.google.com/document/d/1U-AWXkJEDXZFqhBwFlDjyp9bLsVOeeXGYaxa6SZ7KpY/edit?usp=drive_link)\n","\n","* [Logistic Regression with sklearn](https://docs.google.com/document/d/1Xi4fXFROik5Rs6C0d3oIM-OmK3pvw7MwkvM3TJw7vn4/edit?usp=drive_link)\n","\n","* [Deep Learning with pytorch](https://docs.google.com/document/d/1Wm01maZUrSuwdOhuI05uZBtqt5nL5shOGnJ7kTHWl_I/edit?usp=drive_link)\n","\n","* [CNNs with pytorch](https://docs.google.com/document/d/15UV1gVy5J6fzAD5vYyikiprew4erlR9Fop66h89ql0w/edit?usp=drive_link)\n","\n","* [Natural Language Processing I](https://docs.google.com/document/d/1MamYMxe8zlWoiDc0tX2RzUKQULCPVUh-2QtdzRRvzcs/edit?usp=drive_link)\n","\n","* [Natural Language Processing II](https://docs.google.com/document/d/1OoP-sFW6qMk0BzvYMlavgJtiXX9eziTUptlFdzgLfGk/edit?usp=drive_link)\n","\n","* [Natural Language Processing III](https://docs.google.com/document/d/1jrzya_r_97qrmk7RGKqWCPhkHhsKMsMDkrn7YjdZUK0/edit?usp=drive_link)\n","\n","<br>\n","\n","**Before starting, run the code below to import all necessary functions and libraries.**\n"],"metadata":{"id":"mbZXQ3rA3NwL"}},{"cell_type":"code","source":["!pip --quiet install scikit-learn scikit-optimize\n","!pip --quiet install torchview torch graphviz\n","!pip --quiet install fastai\n","!pip --quiet install hyperopt\n","!conda install -q python-graphviz"],"metadata":{"id":"k4wtAc56PwfZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import random\n","from random import choices\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from PIL import Image\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","\n","from fastai.vision.all import *\n","from fastai.text.all import *\n","from fastai.optimizer import Adam\n","\n","import torchvision\n","from torchvision import datasets, transforms\n","from torchvision.datasets import ImageFolder\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import *\n","\n","\n","from skopt import BayesSearchCV\n","from skopt.space import Integer\n","from hyperopt import fmin, tpe, rand, hp, Trials, STATUS_OK\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n","\n","\n","def binary_accuracy(y_pred, y_true):\n","    # Output 0 if y_pred <= 0.5 and 1 if y_pred is > 0.5\n","    y_pred = (y_pred > 0.5).float()\n","    # Returns accuracy\n","    return (y_pred == y_true).float().mean()"],"metadata":{"id":"YAvvLhRIoqYp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"p1\"></a>\n","\n","---\n","## **Part 1: Heart Attack Predictor**\n","---\n","\n","Your goal in this section is to use KNN and a neural network to predict whether a given patient is likely to have a heart attack or not. Specifically, the dataset has:\n","\n","* **Features**: `'age'`, `'sex'`, `'cp'`, `'trestbps'`, `'chol'`, `'fbs'`, `'restecg'`, `'thalach'`, `'exang'`, `'oldpeak'` that describe a variety of health statistics taken by doctors for a given patient.\n","\n","* **Target**: `'heart attack'` which is 0 if the patient has not had a heart attack and 1 if the patient has had a heart attack.\n","\n","\n","<br>\n","\n","**Run the code provided below to import the dataset and split into training and test sets.**"],"metadata":{"id":"LT4CBGqgb5qV"}},{"cell_type":"code","source":["# Load the data into a DataFrame\n","url = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vSa0metcKBFqn-MHLn05vVGWONMlzljcWa-xIM1wJPXIa5kbrmIzGqmWcMh8eKG_ntByF9qqn6Mx3MT/pub?gid=1052859518&single=true&output=csv'\n","df = pd.read_csv(url)\n","\n","# Split the data into training and validation sets\n","X_train, X_valid, y_train, y_valid = train_test_split(\n","    df.drop(columns = 'heart attack'),\n","    df['heart attack'],\n","    test_size = 0.2,\n","    random_state = 42)\n","\n","# Define a custom function for creating a DataLoader from features and target data\n","def create_dataloader(X,y):\n","  X_tensor = torch.tensor(X.to_numpy(), dtype=torch.float32)\n","  y_tensor = torch.tensor(y.to_numpy(), dtype=torch.float32).unsqueeze(1)\n","  dataset = list(zip(X_tensor, y_tensor))\n","  dl = DataLoader(dataset, batch_size=64, shuffle=True)\n","  return dl\n","\n","# Define the DataLoaders\n","train_dl = create_dataloader(X_train, y_train)\n","valid_dl = create_dataloader(X_valid, y_valid)\n","dls = DataLoaders(train_dl, valid_dl)\n","\n","# Define DataLoaders for female subset\n","train_dl_female = create_dataloader(\n","    X_train[X_train['sex']==0],\n","    y_train[X_train['sex']==0])\n","valid_dl_female = create_dataloader(\n","    X_valid[X_valid['sex']==0],\n","    y_valid[X_valid['sex']==0])\n","dls_female = DataLoaders(train_dl_female, valid_dl_female)\n","\n","# Define DataLoaders for male subset\n","train_dl_male = create_dataloader(\n","    X_train[X_train['sex']==1],\n","    y_train[X_train['sex']==1])\n","valid_dl_male = create_dataloader(\n","    X_valid[X_valid['sex']==1],\n","    y_valid[X_valid['sex']==1])\n","dls_male = DataLoaders(train_dl_male, valid_dl_male)"],"metadata":{"id":"n7POsVjustfi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #1.1: Use a KNN classifier**\n","\n","\n","Train a KNN model to perform this task. Try multiple values of K (`n_neighbors`) to achieve the highest performance you can on the training and validation data. Consider using random search, grid search, or bayesian optimization to find the best value of K."],"metadata":{"id":"ftzG-nSHb5qX"}},{"cell_type":"code","source":["# Define the KNN Classifier\n","\n","\n","# Train the model\n","\n","\n","# Make predictions\n","y_pred = # COMPLETE THIS CODE\n","\n","train_accuracy = accuracy_score(y_train, # COMPLETE THIS CODE\n","valid_accuracy = accuracy_score(# COMPLETE THIS CODE\n","\n","print(f\"Training accuracy: {train_accuracy:.4f}\")\n","print(f\"Validation accuracy: {valid_accuracy:.4f}\")\n","\n","# Display confusion matrix\n","cm = confusion_matrix(y_valid, y_pred, labels=knn.classes_)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=knn.classes_)\n","disp.plot()\n","\n","plt.xticks(rotation = 90)\n","plt.show()"],"metadata":{"id":"XwAtu7dLb5qX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #1.2: Use a fully connected neural network**\n","\n","Train a fully connected network to perform this task. Change model architechure and hyperparameters to improve the model, particularly using regularization and hyperparameter tuning techniques. What's the highest accuracy you are able to achieve on the validation set?"],"metadata":{"id":"8-tUsGicSSHt"}},{"cell_type":"code","source":["# Hyperparameter search\n","# Define the objective function to minimize (Hyperopt minimizes the loss, so we negate accuracy)\n","def objective(params):\n","\n","    # COMPLETE THIS CODE\n","\n","\n","\n","# Define the search space\n","space = {\n","    # COMPLETE THIS CODE\n","}\n","\n","# Perform optimization\n","trials = Trials()\n","best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=20, trials=trials)\n","\n","\n","\n","\n","# Train the best model\n","# Get the best parameters and best score\n","best_params = {k: best[k] for k in best}\n","best_score = -trials.best_trial['result']['loss']\n","\n","\n","model = nn.Sequential(\n","  # COMPLETE THIS CODE\n",")\n","\n","# COMPLETE THIS CODE\n","\n","learn = Learner(dls, model, loss_func=loss_func, metrics=binary_accuracy, # COMPLETE THIS CODE\n","learn.fit(# COMPLETE THIS CODE\n","\n","\n","\n","# Calculate training accuracy\n","train_loss, train_accuracy = learn.validate(# COMPLETE THIS CODE\n","print(f\"Training accuracy: {train_accuracy:.4f}\")\n","\n","# Calculate validation accuracy\n","valid_loss, valid_accuracy = learn.validate(# COMPLETE THIS CODE\n","print(f\"Validation accuracy: {valid_accuracy:.4f}\")"],"metadata":{"id":"sUW9S3JbSSHu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Reflection Questions**\n","* Which of your models performed better?\n","* Does deep learning always outperform traditional ML?\n","* What kinds of problems are best-suited for deep learning?"],"metadata":{"id":"fABlSPD35s_5"}},{"cell_type":"code","source":["'''\n","# WRITE YOUR RESPONSES HERE\n","''';"],"metadata":{"id":"cqAeRftH6Ile"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #1.3: Evaluate for Females and Males Separately**\n","\n","\n","Now, evaluate both of your models for females and males separately to see if there's any difference in the performance of your high performing models. You can just print the validation accuracy."],"metadata":{"id":"nrrgQR8LV5im"}},{"cell_type":"markdown","source":["##### **1. Evaluate KNN for Female vs. Male.**"],"metadata":{"id":"XnidPG7SWVM1"}},{"cell_type":"code","source":["female_rows = X_valid['sex']==0\n","male_rows = # COMPLETE THIS CODE\n","\n","female_preds = knn.predict(X_valid[female_rows])\n","male_preds = # COMPLETE THIS CODE\n","\n","accuracy_female = accuracy_score(y_valid[female_rows], # COMPLETE THIS CODE\n","accuracy_male = accuracy_score( # COMPLETE THIS CODE\n","print(f\"Female validation accuracy: {accuracy_female:.4f}\")\n","print(f\"Male validation accuracy: {accuracy_male:.4f}\")"],"metadata":{"id":"o_zPMhQ_WThy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### **2. Evaluate NN for Female vs. Male.**\n","\n","**NOTE**: We have already created a separated female and male train and validation data loader that you can use called: `train_dl_female`, `valid_dl_female`, `train_dl_male`, and `valid_dl_male`."],"metadata":{"id":"VGtB57rmWaxY"}},{"cell_type":"code","source":["# Calculate female training and validation accuracy\n","_, train_accuracy_female = learn.validate(# COMPLETE THIS CODE\n","print(f\"Female training accuracy: {train_accuracy_female:.4f}\")\n","\n","_, valid_accuracy_female = learn.validate(# COMPLETE THIS CODE\n","print(f\"Female validation accuracy: {valid_accuracy_female:.4f}\")\n","\n","\n","# Calculate male training and validation accuracy\n","_, train_accuracy_male = learn.validate(# COMPLETE THIS CODE\n","print(f\"Male training accuracy: {train_accuracy_male:.4f}\")\n","\n","_, valid_accuracy_male = learn.validate(# COMPLETE THIS CODE\n","print(f\"Male validation accuracy: {valid_accuracy_male:.4f}\")"],"metadata":{"id":"4z6DGZrgAwkf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #1.4: Examine why**\n","\n","\n","You likely saw a noticeable difference in the performance between females and males for both models. Examine why this might be by doing the following:\n","\n","1. Plot a bar chart of the number of males vs. females in this data.\n","2. Plot a grouped bar chart of the number of males and females that did not have a heart attack vs. those that did."],"metadata":{"id":"6N5GMFCrXAzJ"}},{"cell_type":"markdown","source":["##### **1. Plot a bar chart of the number of males vs. females in this data.**"],"metadata":{"id":"ywhcdBEsXmNA"}},{"cell_type":"code","source":["plt.bar(['Female', 'Male'], df['sex'].# COMPLETE THIS CODE\n","\n","# COMPLETE THIS CODE"],"metadata":{"id":"CWo0nWGIXmNA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### **2. Plot a grouped bar chart of the number of males and females that did not have a heart attack vs. those that did.**"],"metadata":{"id":"8qrl7MU_XrP_"}},{"cell_type":"code","source":["df_female = df[df['sex'] == 0]\n","df_male = df[df['sex'] == 1]\n","\n","plt.bar([-0.1, 0.9], # COMPLETE THIS CODE\n","plt.bar([0.1, 1.1], # COMPLETE THIS CODE\n","\n","plt.xticks(ticks = [0, 1], labels = ['No Heart Attack', 'Heart Attack'], fontsize = 'x-large')\n","plt.title('Breakdown of Heart Attacks by Sex', fontsize = 'x-large')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"SGmMV3ysXrQA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #1.5: What if we blind the models to this variable?**\n","\n","\n","A common approach to avoid bias is to take a \"blind\" approach, in which we remove the biased variable from the equation. In this case, we'll do that by training new models using data without the `'sex'` column. Specifically,\n","\n","1. Train and evaluate an KNN Classifier on the blind data.\n","2. Train and evaluate a NN on the blind data.\n","\n","In both cases, evaluate the models separately on the female and male rows just as you did in Problem #3.\n","\n","<br>\n","\n","**Run the code below before starting to create the blind data.**"],"metadata":{"id":"0aQFHl-MZdcn"}},{"cell_type":"code","source":["X_train_blind = X_train.drop(columns=['sex'])\n","X_valid_blind = X_valid.drop(columns=['sex'])"],"metadata":{"id":"-EVHPr-iY94o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### **1. Train and evaluate a KNN Classifier on the blind data.**"],"metadata":{"id":"tq6Ky9slaein"}},{"cell_type":"code","source":["# COMPLETE THIS CODE"],"metadata":{"id":"8TtIZIFpaeio"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### **2. Train and evaluate a neural net on the blind data.**\n","\n","First, run the cell below to set up the dataloaders.\n"],"metadata":{"id":"4ZxYlfDsai09"}},{"cell_type":"code","source":["train_dl_blind = create_dataloader(X_train_blind, y_train)\n","valid_dl_blind = create_dataloader(X_valid_blind, y_valid)\n","dls_blind = DataLoaders(train_dl_blind, valid_dl_blind)"],"metadata":{"id":"ESOHI32F4YAb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Perform hyperparameter search\n","\n","\n","# Train the model\n","\n","\n","# Calculate overall training and validation accuracy\n","\n","\n","# Calculate female training and validation accuracy\n","\n","\n","# Calculate male training and validation accuracy\n"],"metadata":{"id":"NtU2LwmPai0-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Potential Future Work**\n","---\n","\n","This is an unfortunately common case of biased data, specifically *unbalanced data*, leading to potentially harmful results. The attempt at blinding the models to the sex of the patient likely provided little to no help. Oftentimes, bias runs deeper than the most obvious variables and may be correlated with others in ways that humans and especially advanced ML algorithms can still pick up on. Consider some of the following ideas for improving on these results:\n","\n","* Training models separately for male and female and data.\n","\n","* Using a statistical methods for balancing the data. For instance, upsampling and downsampling are common first approaches to tackling this problem.\n","\n","* Find a dataset that is more balanced to begin with. In an ideal world, we would make sure that the data is balanced (representative) upon collection."],"metadata":{"id":"8Y87uet9b5qb"}},{"cell_type":"markdown","source":["<a name=\"p2\"></a>\n","\n","---\n","## **Part 2: Amazon Review Analysis**\n","---\n","\n","Your goal in this section is to use a neural network to predict the satisfaction of a customer based on their rating from 0.5 to 5. You can use a model of your choice. This is a difficult dataset, what's the highest accuracy you are able to achieve?\n","\n","We will provide a column of the dataframe with text with stopwords removed. You can use either the 'text' or the 'text_without_stopwords' for your model.\n","\n","<br>\n","\n","**Run the code provided below to import the dataset.**"],"metadata":{"id":"idga37M2FsMR"}},{"cell_type":"code","source":["df = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vT3fAwK4iEaWvsgy5XjbbwVxyzVpQj3En2hk7hO9D5giyk8zvx9xfOP0aU4o9p0ujvaeV4Tcfi-JnyN/pub?gid=103697572&single=true&output=csv')\n","df['text'] = df['title'] + ' ' + df['review']\n","\n","# Define stopwords\n","import sklearn.feature_extraction.text as text\n","stop = text.ENGLISH_STOP_WORDS\n","\n","# Remove stopwords from text column\n","df['text_without_stopwords'] = df['text'].apply(lambda x: ' '.join([word for word in str(x).split() if word not in stop]) if isinstance(x, str) else x)\n","\n","# Show the results\n","df[['text', 'text_without_stopwords','rating']].head(2)"],"metadata":{"id":"Whi6jAQ3i33-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.iloc[3:5]"],"metadata":{"id":"9oYLZy97S3rL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Step #1: Import and split data into training and validation sets**\n","\n","\n","Use TextDataLoaders to load and split the data.\n"],"metadata":{"id":"HuGSwhDc0e1c"}},{"cell_type":"code","source":["dls = TextDataLoaders.from_df(\n","    df,\n","    text_col=# COMPLETE THIS CODE\n","    label_col='rating',\n","    valid_pct=0.2,\n","    bs=#choose a batch size,\n","    seq_len=#choose a sequence length,\n","    device=device\n",")"],"metadata":{"id":"mQfinV3ibCRl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Step #2: Determine the input dimension of your data**\n","\n","\n","Print the length of the vocabulary."],"metadata":{"id":"OPjqwm5rbnz1"}},{"cell_type":"code","source":["# COMPLETE THIS CODE"],"metadata":{"id":"7VtKjWWKb1Pq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Steps #3-6: Build the neural network**\n","\n","\n","You can use model of your choice. We recommend using the models *with* embeddings. You may also use a pre-trained models."],"metadata":{"id":"No1NTRVIbv6a"}},{"cell_type":"code","source":["# COMPLETE THIS CODE"],"metadata":{"id":"2UN3xIu5b06R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Step #7: Train the model**\n"],"metadata":{"id":"G-tcnWQ9b0ZN"}},{"cell_type":"code","source":["# COMPLETE THIS CODE"],"metadata":{"id":"5uWynb3Hb-_G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Step #8: Evaluate the model**\n"],"metadata":{"id":"kganDvF7b_gk"}},{"cell_type":"code","source":["# COMPLETE THIS CODE"],"metadata":{"id":"3QfhkVSLcF0n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Potential Future Work**\n","---\n","\n","Hopefully, through some trial and error and using the variety of tools you have learned at this point, you were able to create a pretty accurate model. If you are interested in going further with this dataset, here are some ideas to consider:\n","* Compare results with and without stopwords. Which performed better?\n","\n","* How does the model perform when given just the title or just the review?\n","\n","* Can you treat this as a regression problem?\n","\n","* To get further insights into the data or even what your model is doing, you could create a wordcloud using this library: https://pypi.org/project/wordcloud/ (or others)."],"metadata":{"id":"lNLqep67Chp_"}},{"cell_type":"markdown","source":["<a name=\"p3\"></a>\n","\n","---\n","## **Part 3: Semantic Segmentation with U-Net**\n","---\n","\n","In this project, we'll complete a semantic segmentation task on a subset of the CamVid dataset. The CamVid dataset is a relatively small dataset containing images of street scenes, with pixel-level annotations for 32 semantic classes.\n","\n","We'll use the U-Net architecture with a pretrained ResNet-34 \"backbone\" to perform the segmentation.\n","\n","U-Net is a semantic segmentation architecture designed in a U-shape, composed of an encoder (contracting path) and a decoder (expanding path). The encoder is a typical convolutional neural network (CNN) that uses convolution and pooling layers. The decoder, a \"reverse\" CNN, recovers spatial information using up-convolution (also called transpose convolution) layers and upsampling layers.\n","\n","Using a ResNet-34 backbone means that the encoder of the U-Net is replaced with a pretrained ResNet-34 architecture. This takes advantage of transfer learning and a more efficient design with residual connections, resulting in faster convergence and better performance for the segmentation task.\n","\n","<br>\n","\n","**Run the code provided below to import the dataset.**"],"metadata":{"id":"FFgXZtSGO1AP"}},{"cell_type":"code","source":["path = untar_data(URLs.CAMVID_TINY)\n","\n","def get_y_fn(x):\n","    return path/'labels'/f'{x.stem}_P{x.suffix}'\n","\n","codes = np.loadtxt(path/'codes.txt', dtype=str)\n","\n","dls = SegmentationDataLoaders.from_label_func(\n","    path,\n","    get_image_files(path/\"images\"),\n","    get_y_fn,\n","    codes=codes,\n","    bs=8,\n","    item_tfms=Resize(460),\n","    batch_tfms=[*aug_transforms(), Normalize.from_stats(*imagenet_stats)]\n",")"],"metadata":{"id":"BG9TY971VqBU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #3.1: Create the Learner**\n","\n","\n","\n","You'll be using the [`unet_learner()`](https://docs.fast.ai/vision.learner.html#unet_learner) function. We can provide as an input the choice to use Resnet34 as the backbone. Instantiate the learner with the following inputs:\n","* `dls`\n","* `resnet34`\n","* `metrics=Dice()`\n","* `wd=1e-2`\n","\n","Finally, you can use a method called `to_fp16()` at the end of the model definition to have the model work in single precision, which can save a lot of training time.\n","\n","The Dice metric is better suited for segmentation problems. It is a coefficient that compares similarity: low numbers are for poor similarity. You should aim to increase this metric as much as possible when hyperparameter tuning."],"metadata":{"id":"tjqhE1YNbMC1"}},{"cell_type":"code","source":["learn = # COMPLETE THIS CODE"],"metadata":{"id":"lQbrppkha5-p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #3.2**\n","\n","\n","\n","Train your model using either `learn.fit()` or `learn.fine_tune()`. Remember if you use `learn.fit()` you will need to use `learn.freeze()` before."],"metadata":{"id":"zZCOANTTcnVa"}},{"cell_type":"code","source":["# COMPLETE THIS CODE"],"metadata":{"id":"LMy3Sltta3OP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #3.3: View results**\n","\n","\n","Run the code below to visualize your results. How did your model perform? How much can you improve the segmentation masks through hyperparameter tuning?"],"metadata":{"id":"OA9nEAIneDnQ"}},{"cell_type":"code","source":["learn.show_results(max_n=2, figsize=(10, 10))"],"metadata":{"id":"iV8hJ5q5X7HM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_path = 'path/to/your/image.jpg'\n","img = PIL.Image.open(image_path)\n","pred_mask, _, _ = learn.predict(img)\n","pred_mask.show(figsize=(5, 5))"],"metadata":{"id":"YkRawzYKX7td"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Potential Future Work**\n","---\n","\n","Image segmentation is a challenging task that requires balancing model complexity, dataset size, and computation time. Here are some ideas for improving on these results:\n","\n","* Training with the full CamVid dataset or another image segmentation dataset.\n","\n","* Fine-tuning the pre-trained model. This project used a pre-trained ResNet34 as the backbone of the UNet model. However, there are many other pre-trained models available that may work better for this specific task. Consider trying a different model, such as ResNet50 or VGG16, and fine-tuning the model on the segmentation dataset.\n"],"metadata":{"id":"aXBZ3BdHdSQX"}},{"cell_type":"markdown","source":["<a name=\"p4\"></a>\n","\n","---\n","## **Part 4: Generating Wikipedia Entries**\n","---\n","\n","In this section, you will apply what you learned about generating text by training a model on 30,000 sentences from Wikipedia as of 2021. This text has been downloaded from [https://wortschatz.uni-leipzig.de/en/download/English](https://wortschatz.uni-leipzig.de/en/download/English), which also contains the entries in several other languages as well as other corpora from the internet.\n","\n","<br>\n","\n","\n","**Run the code provided below to import the dataset.**"],"metadata":{"id":"bqEVl7sfwa-w"}},{"cell_type":"code","source":["path = untar_data(URLs.WIKITEXT_TINY)\n","train_df = pd.read_csv(path/'train.csv', header=None, names=['text'])\n","dls = TextDataLoaders.from_df(train_df,\n","                              text_col='text',\n","                              is_lm=True,\n","                              valid_pct=0.1,\n","                              bs=64)"],"metadata":{"id":"DNmnMUyiJaiK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #4.1: Define a pre-trained language model learner**\n","\n","\n","\n","You'll use the function `language_model_learner()` which works very similarly to `text_classifier_learner()`. Pass the following inputs to the model:\n","* `dls`\n","* `AWD_LSTM`\n","* `metrics=[accuracy, Perplexity()]`,\n","* `wd=0.1`\n","\n","Finally, you can use a method called `to_fp16()` at the end of the model definition to have the model work in single precision, which can save a lot of training time."],"metadata":{"id":"QRxpJ9GZOf4J"}},{"cell_type":"code","source":["learn = language_model_learner(dls, AWD_LSTM, metrics=[accuracy, Perplexity()], wd=0.1).to_fp16()"],"metadata":{"id":"9S9tuDWqJdlk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #4.2: Train your model**\n"],"metadata":{"id":"pqVhuXDphVeH"}},{"cell_type":"code","source":["learn.fine_tune(5)"],"metadata":{"id":"LPFdMyK_Jinp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If you are happy with your model, we recommend saving it so you don't have to re-train it again later. The code for saving the model is provided for you below."],"metadata":{"id":"q5ze7vkFQBW0"}},{"cell_type":"code","source":["learn.save('fine_tuned_wikitext_tiny')"],"metadata":{"id":"5RGWqAmYHRAA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Below is the code to load the model again. Only the model weights are saved, so when using `learn.load()` make sure you first define the model again."],"metadata":{"id":"UUVPOiJMP4Zi"}},{"cell_type":"code","source":["learn.load('fine_tuned_wikitext_tiny')"],"metadata":{"id":"yw6vn1A8LekZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #4.3: Generate Text**\n","\n","\n","We have provided a function below that you can use to generate text. The inputs are:\n","* prompt -> A string that you want the model to generate text at the end of\n","* n_words -> The number of words the model should generate\n","* temperature -> A parameter that represents the amount of \"randomness\" in the model's response\n","\n","**Run the cell below to set up the function.**"],"metadata":{"id":"JgQEIXUZhjlZ"}},{"cell_type":"code","source":["def generate_text(prompt, n_words=20, temperature=1.0):\n","    return learn.predict(prompt, n_words, temperature=temperature)"],"metadata":{"id":"QAgmRAqIHT0S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"Chicago is a place where\"\n","predicted_text = generate_text(prompt,30,0.75)\n","print(predicted_text)"],"metadata":{"id":"xjkc3tR7MUGq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Potential Future Work**\n","---\n","\n","Text generation can be a tricky task that can be extremely dependent on the framing of the problem, the dataset available, the models used, and more. Consider some of the following ideas for improving on these results:\n","\n","* Training with the full Wikitext dataset or another language model dataset.\n","\n","* Hyperparameter tuning or using a different pre-trained model, maybe a transformer model.\n","\n","* Can you think of how modify this code to make a chatbot?"],"metadata":{"id":"GfT6-HV_uF5r"}},{"cell_type":"markdown","source":["# End of notebook\n","---\n","© 2024 The Coding School, All rights reserved"],"metadata":{"id":"7dzC09dLlEhm"}}]}