{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Fyptwc--b9Yu2pXccV3DeJoDyBcraYDd","timestamp":1682411072336},{"file_id":"1PupwQQp6LZFfMIPgWLKlWXrYDgLuA5sj","timestamp":1678127175525}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# **Day 8: Embeddings**\n","---\n","\n","### **Description**\n","In today's lab, we have another text classification task, but this time we will be using **embeddings.** For this project, we will be working with a dataset of BBC News articles classified by topic.\n","\n","<br>\n","\n","### **Lab Structure**\n","\n","**Part 1**: [Review](#p1)\n",">\n",">**Part 1.1**: [Tokenization and Vectorization with sklearn](#p1.1)\n",">\n",">**Part 1.2**: [Tokenization and Vectorization with TextDataLoaders](#p1.2)\n","\n","**Part 2**: [Embeddings](#p2)\n","\n","\n","**Part 3**: [News Classification with a Simple Neural Net with Embedding](#p3)\n","\n","**Part 4**: [News Classification with a CNN with Embedding](#p4)\n","\n","**Part 5**: [[ADDITIONAL PRACTICE] Sentiment Analysis with IMDB Movie Review](#p5)\n","\n","<br>\n","\n","### **Goals**\n","By the end of this lab, you will:\n","* Understand how tokenization and vectorization works when using TextDataLoaders\n","* Understand how to apply embedding layers in models.\n","* Compare a fully connected network to a CNN for text classification with embeddings.\n","\n","<br>\n","\n","### **Cheat Sheets**\n","[Natural Language Processing II](https://docs.google.com/document/d/1OoP-sFW6qMk0BzvYMlavgJtiXX9eziTUptlFdzgLfGk/edit)\n","\n","<br>\n","\n","**Before starting, run the code below to import all necessary functions and libraries.**\n"],"metadata":{"id":"mbZXQ3rA3NwL"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset, DataLoader\n","import warnings\n","warnings.filterwarnings('ignore')\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from fastai.text.all import *\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # PyTorch v0.4.0"],"metadata":{"id":"YAvvLhRIoqYp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"p1\"></a>\n","\n","---\n","## **Part 1: Review**\n","---\n","\n"],"metadata":{"id":"BVkHj35fHI8I"}},{"cell_type":"markdown","source":["<a name=\"p1.1\"></a>\n","\n","---\n","### **Part 1.1: Tokenization and Vectorization with sklearn**\n","---\n","\n","**Run the cell below to load a simple corpus for us to work with.**"],"metadata":{"id":"YTqvDkY_HrMF"}},{"cell_type":"code","source":["# Define a collection of text documents\n","corpus = [\n","       \"One Cent, Two Cents, Old Cent, New Cent: All About Money (Cat in the Hat's Learning Library\",\n","       \"Inside Your Outside: All About the Human Body (Cat in the Hat's Learning Library)\",\n","       \"Oh, The Things You Can Do That Are Good for You: All About Staying Healthy (Cat in the Hat's Learning Library)\",\n","       \"On Beyond Bugs: All About Insects (Cat in the Hat's Learning Library)\",\n","       \"There's No Place Like Space: All About Our Solar System (Cat in the Hat's Learning Library)\"\n","]"],"metadata":{"id":"R0mghxBvHI8J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #1.1.1: Create a CountVectorizer object**\n","\n"],"metadata":{"id":"FSG6esqDHI8J"}},{"cell_type":"code","source":["vectorizer = #FILL IN CODE HERE"],"metadata":{"id":"FRFEBNhiHI8J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #1.1.2: Fit the vectorizer to the corpus**\n","\n"],"metadata":{"id":"YAT7clzeHI8K"}},{"cell_type":"code","source":[],"metadata":{"id":"Z0UXhoLpHI8K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #1.1.3: Transform the corpus into a matrix of token counts**\n","\n"],"metadata":{"id":"Z8arRrHDHI8K"}},{"cell_type":"code","source":["# Transform the corpus into a matrix of token counts\n","# WRITE YOUR CODE HERE\n","\n","# Print the resulting matrix\n","print(X.toarray())"],"metadata":{"id":"sQXkSV0nH-Un"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #1.1.4: Print the tokens**\n","\n","Use `get_feature_names_out()` to print the tokens.\n"],"metadata":{"id":"97kwXIW6HI8L"}},{"cell_type":"code","source":[],"metadata":{"id":"fo17iHTTHI8L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Compare the tokens, the matrix, and the corpus. Do you see how each sentence is represented in the matrix?"],"metadata":{"id":"4d_cmQjP7xKT"}},{"cell_type":"markdown","source":["<a name=\"p1.2\"></a>\n","\n","---\n","### **Part 1.2: Tokenization and Vectorization with TextDataLoaders**\n","---\n","\n","Last time, we learned how to tokenize and vectorize data using `sklearn`'s `CountVectorizer()`. However, we don't always have to do this step manually. When loading data with fast.ai's `TextDataLoaders`, the DataLoader handles the tokenization and vectorization for us. Let's take a look.\n","\n","**Run the code below to load the BBC News data into a pandas DataFrame.**\n"],"metadata":{"id":"_X9DchFl6uZg"}},{"cell_type":"code","source":["dataset = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vRRiQ1DUkUxk31YpaHA2i9QtwGq_VGXiy86z7l3aT9v5zoB6M7a-2M2qlYckr1C_ZG6StBELlU_hD3S/pub?output=csv')"],"metadata":{"id":"jxb7Iynu6q2a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #1.2.1: Load the data using TextDataLoaders**\n","\n","Use the following parameters:\n","* `dataset`\n","* `text_col='text'`\n","* `label_col='category'`\n","* `valid_pct=0.2`\n","* `bs=64`\n","* `seq_len=100`\n"],"metadata":{"id":"ZZPhj3RW68Kq"}},{"cell_type":"code","source":["dls = TextDataLoaders.from_df(\n","    #FILL IN CODE HERE\n",")"],"metadata":{"id":"xtKR2kbk8zpw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #1.2.2: Print the vocabulary**\n","\n","\n","The `vocab` attribute of the DataLoaders object contains the vocabulary of the data and of the labels.\n","\n","Use the first element of the `vocab` attribute of the DataLoaders object to print the vocabulary of the data."],"metadata":{"id":"Yt03nQ5Z7D2u"}},{"cell_type":"code","source":[],"metadata":{"id":"ISVa9A52832F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #1.2.3: Print the labels**\n","\n","\n","Use the second element of the `vocab` attribute to print the labels."],"metadata":{"id":"PSVTgBD67QFQ"}},{"cell_type":"code","source":[],"metadata":{"id":"mxfzW9c484nz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #1.2.4: View vectorized data**\n","---\n","\n","TextDataLoaders assigns a unique integer ID to each token in the vocabulary, while preserving the order of the tokens.\n","\n","Use `dls.one_batch()` to pull one batch of the data and view the first instance."],"metadata":{"id":"K7jj8YPZ7hFR"}},{"cell_type":"code","source":["xb, yb = # FILL IN CODE HERE"],"metadata":{"id":"11eK9s-y86TR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #1.2.5: Decode vectorized data**\n","---\n","\n","Use `dls.show()` to decode the numeric data.\n","\n","*Hint: Pass a tuple to the function.*"],"metadata":{"id":"BqZeCFZsrwq3"}},{"cell_type":"code","source":[],"metadata":{"id":"n93S7X6N8kzm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","<center>\n","\n","#### **Back to lecture**\n","\n","---"],"metadata":{"id":"cPPib_SElbqQ"}},{"cell_type":"markdown","source":["<a name=\"p2\"></a>\n","\n","---\n","## **Part 2: Embeddings**\n","---"],"metadata":{"id":"wY5xiH-hwBef"}},{"cell_type":"markdown","source":["#### **Problem #2.1: Create an embedding layer in PyTorch**\n","\n","\n","We can use PyTorch to create embeddings. Create an embedding layer. The first input will be the vocab size, and the second input is the embedding dimension. Set the embedding dimension to 50."],"metadata":{"id":"f3h5sb5sxQmI"}},{"cell_type":"code","source":["# Create an embedding layer with 50 dimensions\n","vocab_size = # FILL IN CODE HERE\n","embedding = nn.Embedding(vocab_size, # FILL IN CODE HERE)"],"metadata":{"id":"96lSDBvmxth7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #2.2: Apply the embedding**\n","\n","\n","Earlier, we pulled one batch of the data and saved the data `xb` and labels `yb`. Apply the embedding to the batch of data `xb`. Compare this numerical representation to the representation in Problem #4."],"metadata":{"id":"4tPOqgYlyH1z"}},{"cell_type":"code","source":["# Apply the embedding layer\n","embedded = # FILL IN CODE HERE\n","\n","# Print the first embedded instance of the data\n","embedded[0]"],"metadata":{"id":"wWptl2WpyYba"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Run the code below to visualize the embedding.**\n","\n","You can change the first and second dimension for plotting."],"metadata":{"id":"PUdBt4bIzr-P"}},{"cell_type":"code","source":["first_dimension = 0\n","second_dimension = 1\n","\n","# Detach the tensor from the computational graph (preparing for plotting)\n","embedded = embedded.detach()\n","\n","# Plot the embeddings\n","plt.figure(figsize=(10, 10))\n","plt.scatter(\n","    embedded[:, :, first_dimension].numpy().flatten(),\n","    embedded[:, :, second_dimension].numpy().flatten(),\n","    s=10)\n","plt.title('Embedding Visualization')\n","plt.xlabel('Dimension 1')\n","plt.ylabel('Dimension 2')\n","plt.show()"],"metadata":{"id":"ULSUaj1fwqET"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","<center>\n","\n","#### **Back to lecture**\n","\n","---"],"metadata":{"id":"H1s-_tRTz90o"}},{"cell_type":"markdown","source":["<a name=\"p3\"></a>\n","\n","---\n","## **Part 3: News Classification with a Simple Neural Net with Embedding**\n","---"],"metadata":{"id":"kV7du9rJBYuz"}},{"cell_type":"markdown","source":["#### **Problem #3.1**\n","\n","\n","\n","Fill in the code below for a fully connected network of your own design. Ensure you have the correct number of inputs and outputs.\n","\n","Set the embedding dimension to 200."],"metadata":{"id":"ZbhgBwEPCm8P"}},{"cell_type":"code","source":["embed_size = # FILL IN CODE HERE\n","\n","model = nn.Sequential(\n","    nn.Embedding( # FILL IN CODE HERE\n","    nn.AdaptiveAvgPool2d((1, embed_size)),\n","    nn.Flatten(),\n","    nn.Linear(embed_size, # FILL IN CODE HERE\n","    # ADD THE REST OF YOUR LAYERS\n",")"],"metadata":{"id":"qrIklOq1CmS0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #3.2**\n","\n","Create a Learner object and fit the model. Since this is a multiclass classification problem, you will use `nn.CrossEntropyLoss()` and accuracy as the metric. Choose your own hyperparameters (5 epochs and a learning rate of 0.001 is a good start.)"],"metadata":{"id":"fuGPjq1bLSNA"}},{"cell_type":"code","source":["# Create a Learner and train the model\n"],"metadata":{"id":"rjiBv6JzLk4C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #3.3**\n","\n","Now, evaluate the model for both the training and validation sets.\n"],"metadata":{"id":"JSOOynZqL0xo"}},{"cell_type":"code","source":["# Evaluate the training set\n","\n","\n","# Evaluate the test set\n"],"metadata":{"id":"HqPSJlsSL7_E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **How did your model perform? Try to improve your result with hyperparameter tuning.**\n","\n","\n"],"metadata":{"id":"zkn0RBS0DHPo"}},{"cell_type":"markdown","source":["---\n","\n","<center>\n","\n","#### **Back to lecture**\n","\n","---"],"metadata":{"id":"9l2YAKbUv7JW"}},{"cell_type":"markdown","source":["<a name=\"p4\"></a>\n","\n","---\n","## **Part 4: News Classification with a CNN with Embedding**\n","---\n","\n"],"metadata":{"id":"r4_U4kWylKLw"}},{"cell_type":"markdown","source":["#### **Problem #4.1**\n","\n","\n","Let's build a CNN model with an embedding layer. In order for the output of the embedding layer to be the right dimensions for the convolutional layer, we've provided a custom module that transposes the last two dimensions.\n","\n","Remember, we've introduced a new version of the max pooling layer. The previous one specifies the pool size, and requires us to keep track of the output sizes:\n","* `nn.MaxPool1d(2)`\n","\n","For the new one, we just specify what size *output* we would like:\n","* `nn.AdaptiveMaxPool1d(10)`\n","\n","<br>\n","\n","Define a CNN with the following layers:\n","\n","Block 1:\n","* A convolutional layer with 300 outputs, kernel size of 11, `padding='same'`, and ReLU activation.\n","* A adaptive max pooling layer with an output size of 10\n","\n","Block 2\n","* A convolutional layer with 150 outputs, kernel size of 11, `padding='same'`, and ReLU activation.\n","* A adaptive max pooling layer with an output size of 1\n","\n","Finally, add:\n","* A Flatten layer\n","* A linear layer with 20 outputs and ReLU activation\n","* The output layer"],"metadata":{"id":"1AAVJHkuE3nM"}},{"cell_type":"code","source":["# To prepare the embedding layer for the convolutional layer, we need\n","# to define a custom module to transpose the last two dimensions.\n","class Transpose(nn.Module):\n","    def forward(self, x):\n","        return x.transpose(1, 2)\n","\n","model = nn.Sequential(\n","    nn.Embedding(vocab_size, embed_size),\n","    Transpose(),\n","    nn.Conv1d(embed_size, # FILL IN CODE HERE\n","    # ADD THE REST OF THE LAYERS\n",")\n"],"metadata":{"id":"YVE6HlQ4E3nc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #4.2**\n","\n","\n","Create a Learner object and fit the model. Since this is a multiclass classification problem, you will use `nn.CrossEntropyLoss()`"],"metadata":{"id":"M__-6lOuE3nc"}},{"cell_type":"code","source":["# Create a Learner and train the model\n"],"metadata":{"id":"OaB29Xv2E3nd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #4.3**\n","\n","\n","Now, evaluate the model for both the training and validation sets.\n"],"metadata":{"id":"r5KifHJwE3ne"}},{"cell_type":"code","source":["# Evaluate the training set\n","\n","\n","# Evaluate the test set\n"],"metadata":{"id":"k_8YoV92E3ne"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **How did your model perform? Try to improve your result with hyperparameter tuning.**"],"metadata":{"id":"Ek_0isWo2evq"}},{"cell_type":"markdown","source":["<a name=\"p5\"></a>\n","\n","---\n","## **[ADDITIONAL PRACTICE] Part 5: Sentiment Analysis with IMDB Movie Reviews**\n","---\n","\n","In this part, we will create a CNN using the IMDB Movie Reviews dataset, which includes movie reviews along with their corresponding sentiment (positive, neutral, negative)."],"metadata":{"id":"nKibmfTE2_ti"}},{"cell_type":"markdown","source":["####**Problem #5.1**\n","\n","**Run the code below to load the IMDB Movie Reviews data into a pandas DataFrame.**\n","\n","View the DataFrame before beginning.\n"],"metadata":{"id":"uCm0qKFK1vik"}},{"cell_type":"code","source":["dataset = pd.read_csv('https://raw.githubusercontent.com/the-codingschool/TRAIN-datasets/main/imdb_reviews/IMDB_Dataset.csv')\n","\n","dataset.head()"],"metadata":{"id":"rNj2Ldgj1vim"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #5.2: Load the data using TextDataLoaders**\n","\n","Use the following parameters:\n","* `dataset`\n","* `text_col='review'`\n","* `label_col='sentiment'`\n","* `valid_pct=0.2`\n","* `bs=64`\n","* `seq_len=100`\n"],"metadata":{"id":"g_Y4IbyP1vim"}},{"cell_type":"code","source":["dls = TextDataLoaders.from_df(\n","    #FILL IN CODE HERE\n",")"],"metadata":{"id":"THlkpKbF1vim"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #5.3: Print the vocabulary**\n","\n","\n","The `vocab` attribute of the DataLoaders object contains the vocabulary of the data and of the labels.\n","\n","Use the first element of the `vocab` attribute of the DataLoaders object to print the vocabulary of the data."],"metadata":{"id":"ei8x8EYV1vin"}},{"cell_type":"code","source":[],"metadata":{"id":"rZ1T95r71vin"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #5.4: Print the labels**\n","\n","\n","Use the second element of the `vocab` attribute to print the labels."],"metadata":{"id":"y4Q4SIgq1vin"}},{"cell_type":"code","source":[],"metadata":{"id":"51mqzF-E1vin"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #5.5: View vectorized data**\n","\n","\n","TextDataLoaders assigns a unique integer ID to each token in the vocabulary, while preserving the order of the tokens.\n","\n","Use `dls.one_batch()` to pull one batch of the data and view the first instance."],"metadata":{"id":"eQqP0Jvr1vin"}},{"cell_type":"code","source":["xb, yb = # FILL IN CODE HERE"],"metadata":{"id":"Q_tPVtmu1vio"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #5.6: Decode vectorized data**\n","\n","\n","Use `dls.show()` to decode the numeric data.\n","\n","*Hint: Pass a tuple to the function.*"],"metadata":{"id":"36fCRV0U1vio"}},{"cell_type":"code","source":[],"metadata":{"id":"8uXi76dI1vio"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #5.7: Create an embedding layer in PyTorch**\n","\n","\n","We can use PyTorch to create embeddings. Create an embedding layer. The first input will be the vocab size, and the second input is the embedding dimension. Set the embedding dimension to 50."],"metadata":{"id":"3QS7xSyS2_tj"}},{"cell_type":"code","source":["# Create an embedding layer with 50 dimensions\n","vocab_size = # FILL IN CODE HERE\n","embedding = nn.Embedding(vocab_size, # FILL IN CODE HERE)"],"metadata":{"id":"SMWdWCmF2_tj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #5.8: Apply the embedding**\n","\n","\n","Earlier, we pulled one batch of the data and saved the data `xb` and labels `yb`. Apply the embedding to the batch of data `xb`. Compare this numerical representation to the representation in Problem #4."],"metadata":{"id":"Ax2V2rrz2_tk"}},{"cell_type":"code","source":["# Apply the embedding layer\n","embedded = # FILL IN CODE HERE\n","\n","# Print the first embedded instance of the data\n","embedded[0]"],"metadata":{"id":"UImbxEMc2_tk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Run the code below to visualize the embedding.**\n","\n","You can change the first and second dimension for plotting."],"metadata":{"id":"yaua-_Sl2_tk"}},{"cell_type":"code","source":["first_dimension = 0\n","second_dimension = 1\n","\n","# Detach the tensor from the computational graph (preparing for plotting)\n","embedded = embedded.detach()\n","\n","# Plot the embeddings\n","plt.figure(figsize=(10, 10))\n","plt.scatter(\n","    embedded[:, :, first_dimension].numpy().flatten(),\n","    embedded[:, :, second_dimension].numpy().flatten(),\n","    s=10)\n","plt.title('Embedding Visualization')\n","plt.xlabel('Dimension 1')\n","plt.ylabel('Dimension 2')\n","plt.show()"],"metadata":{"id":"r8JoCZBg2_tl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #5.9**\n","\n","\n","Let's build a CNN model with an embedding layer. In order for the output of the embedding layer to be the right dimensions for the convolutional layer, we've provided a custom module that transposes the last two dimensions.\n","\n","Remember, we've introduced a new version of the max pooling layer. The previous one specifies the pool size, and requires us to keep track of the output sizes:\n","* `nn.MaxPool1d(2)`\n","\n","For the new one, we just specify what size *output* we would like:\n","* `nn.AdaptiveMaxPool1d(10)`\n","\n","<br>\n","\n","Define a CNN with the following layers:\n","\n","Block 1:\n","* A convolutional layer with 300 outputs, kernel size of 11, `padding='same'`, and ReLU activation.\n","* A adaptive max pooling layer with an output size of 10\n","\n","Block 2\n","* A convolutional layer with 150 outputs, kernel size of 11, `padding='same'`, and ReLU activation.\n","* A adaptive max pooling layer with an output size of 1\n","\n","Finally, add:\n","* A Flatten layer\n","* A linear layer with 20 outputs and ReLU activation\n","* The output layer"],"metadata":{"id":"oTfVQHwp4N58"}},{"cell_type":"code","source":["# To prepare the embedding layer for the convolutional layer, we need\n","# to define a custom module to transpose the last two dimensions.\n","class Transpose(nn.Module):\n","    def forward(self, x):\n","        return x.transpose(1, 2)\n","\n","model = nn.Sequential(\n","    nn.Embedding(vocab_size, embed_size),\n","    Transpose(),\n","    nn.Conv1d(embed_size, # FILL IN CODE HERE\n","    # ADD THE REST OF THE LAYERS\n",")\n"],"metadata":{"id":"NKU9J3pM4N59"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #5.10**\n","\n","\n","Create a Learner object and fit the model. Since this is a multiclass classification problem, you will use `nn.CrossEntropyLoss()`"],"metadata":{"id":"5oohJDwf4N59"}},{"cell_type":"code","source":["# Create a Learner and train the model\n"],"metadata":{"id":"QwPYnlR24N59"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #5.11**\n","\n","\n","Now, evaluate the model for both the training and validation sets.\n"],"metadata":{"id":"etltdorz4N59"}},{"cell_type":"code","source":["# Evaluate the training set\n","\n","\n","# Evaluate the test set\n"],"metadata":{"id":"r5K8-pux4N59"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","#End of notebook\n","---\n","Â© 2024 The Coding School, All rights reserved"],"metadata":{"id":"7dzC09dLlEhm"}}]}