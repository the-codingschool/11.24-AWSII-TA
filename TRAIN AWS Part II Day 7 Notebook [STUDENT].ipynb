{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1PupwQQp6LZFfMIPgWLKlWXrYDgLuA5sj","timestamp":1678127175525}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# **Lab 7: Introduction to Natural Language Processing (NLP)**\n","---\n","\n","### **Description**\n","In today's lab, we will see how to use neural networks for one of the most popular NLP tasks: **text classification**. This will involve applying what you already know about neural nets and new NLP concepts of tokenization and vectorization.\n","\n","For this project, we will be working with the `fetch_20newsgroups` dataset, which is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. Each newsgroup covers a different topic, such as sports, politics, religion, and technology. The documents within each newsgroup were posted by various authors, and cover a wide range of subtopics related to the main theme of the newsgroup.\n","\n","The goal of this project is to build a machine learning model that can accurately classify newsgroup documents based on their content.\n","\n","<br>\n","\n","### **Lab Structure**\n","**Part 1**: [Tokenization and Vectorization](#p1)\n","\n","**Part 2**: [News Group Classification with a Neural Network](#p2)\n",">\n",">**Part 2.1**: [Tokenizing and Vectorizing the News Groups Dataset](#p2.1)\n",">\n",">**Part 2.2**: [Training and Testing a Neural Network](#p2.2)\n","\n","**Part 3**: [News Group Classification with a CNN](#p3)\n","\n","\n","\n","<br>\n","\n","### **Goals**\n","By the end of this lab, you will:\n","* Understand the concept of tokenization in NLP.\n","* Compare a fully connected network to a CNN for text classification.\n","\n","<br>\n","\n","### **Cheat Sheets**\n","[Natural Language Processing I](https://docs.google.com/document/d/1MamYMxe8zlWoiDc0tX2RzUKQULCPVUh-2QtdzRRvzcs/edit?usp=sharing)\n","\n","<br>\n","\n","**Before starting, run the code below to import all necessary functions and libraries.**\n"],"metadata":{"id":"mbZXQ3rA3NwL"}},{"cell_type":"code","source":["from sklearn.datasets import fetch_20newsgroups\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from fastai.text.all import *\n","\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"YAvvLhRIoqYp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"p1\"></a>\n","\n","---\n","## **Part 1: Tokenization and Vectorization**\n","---\n","\n","**Run the cell below to load a simple corpus for us to work with.**"],"metadata":{"id":"_X9DchFl6uZg"}},{"cell_type":"code","source":["# Define a collection of text documents\n","corpus = [\n","    \"This is the first document.\",\n","    \"This is the second document.\",\n","    \"And this is the third document.\",\n","    \"Is this the first document?\",\n","]"],"metadata":{"id":"jxb7Iynu6q2a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #1.1: Create a CountVectorizer object**\n","\n"],"metadata":{"id":"ZZPhj3RW68Kq"}},{"cell_type":"code","source":["vectorizer = #FILL IN CODE HERE"],"metadata":{"id":"xtKR2kbk8zpw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #1.2: Fit the vectorizer to the corpus**\n","\n"],"metadata":{"id":"Yt03nQ5Z7D2u"}},{"cell_type":"code","source":[],"metadata":{"id":"ISVa9A52832F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #1.3: Transform the corpus into a matrix of token counts**\n","\n"],"metadata":{"id":"PSVTgBD67QFQ"}},{"cell_type":"code","source":["# Transform the corpus into a matrix of token counts\n","# WRITE YOUR CODE HERE\n","\n","# Print the resulting matrix\n","print(X.toarray())"],"metadata":{"id":"mxfzW9c484nz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #1.4: Print the tokens**\n","\n","Use `get_feature_names_out()` to print the tokens.\n"],"metadata":{"id":"K7jj8YPZ7hFR"}},{"cell_type":"code","source":[],"metadata":{"id":"11eK9s-y86TR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Compare the tokens, the matrix, and the corpus. Do you see how each sentence is represented in the matrix?"],"metadata":{"id":"4d_cmQjP7xKT"}},{"cell_type":"markdown","source":["<a name=\"p2\"></a>\n","\n","---\n","## **Part 2: News Group Classification with a Neural Network**\n","---\n","\n","\n","The 20 Newsgroups dataset is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. Our task is to classify the articles to the correct newsgroup.\n","\n","<br>\n","\n","**Run the cell below to load the dataset.**"],"metadata":{"id":"kV7du9rJBYuz"}},{"cell_type":"code","source":["# Load the dataset\n","newsgroups_data = fetch_20newsgroups(\n","    subset='train',\n","    remove=('headers', 'footers', 'quotes')\n",")\n","\n","texts = newsgroups_data.data\n","labels = newsgroups_data.target\n","\n","# Split the dataset into training and validation sets\n","texts_train, texts_val, labels_train, labels_val = train_test_split(\n","    texts,\n","    labels,\n","    test_size=0.2,\n","    random_state=42\n",")"],"metadata":{"id":"foasGD-f9a_R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"p2.1\"></a>\n","\n","---\n","### **Part 2.1: Tokenizing and Vectorizing the News Groups Dataset**\n","---"],"metadata":{"id":"ZqaEmJy38VkC"}},{"cell_type":"markdown","source":["#### **Problem #2.1.1: Create the CountVectorizer object**\n","\n","Initialize the vectorizer with the following parameters:\n","* `stop_words='english'`\n","* `max_features=4000`"],"metadata":{"id":"imQT4CJS9nxb"}},{"cell_type":"code","source":[],"metadata":{"id":"WzdtPvyx-ew7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #2.1.2: Fit and transform the training data.**\n","\n"],"metadata":{"id":"G2xgieYx-BIy"}},{"cell_type":"code","source":[],"metadata":{"id":"-AAplhm8-fQa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #2.1.3: Transform the validation data.**"],"metadata":{"id":"zuJwVtiL-G0y"}},{"cell_type":"code","source":[],"metadata":{"id":"kmk0hhfd-dbz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### **Run the code below to print out the shapes of each BoW matrix and a sample of the vocabulary.**"],"metadata":{"id":"KPeFKyUx_RX4"}},{"cell_type":"code","source":["# Show the shape of the BoW matrices\n","print(\"Shape of the training BoW matrix:\", X_train_bow.shape)\n","print(\"Shape of the validation BoW matrix:\", X_valid_bow.shape)\n","\n","print(L(vectorizer.get_feature_names_out()[2000:2100]))"],"metadata":{"id":"xLcL6cn5JU0t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #2.1.4: Choose a random document and print out its BoW representation.**\n","\n","Then use `get_feature_names_out()` to determine what some of the words are."],"metadata":{"id":"_iLEJ5_vAUzH"}},{"cell_type":"code","source":["random_doc_idx = # You can choose any index\n","print(\"BoW representation of a random document:\\n\", X_train_bow[random_doc_idx])"],"metadata":{"id":"XOm2QWcxAtEg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use get_feature_names_out() to explore your results"],"metadata":{"id":"eVuwXMMLAtgH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"p2.2\"></a>\n","\n","---\n","### **Part 2.2: Training and Testing a Neural Network**\n","---\n","\n","At this point, we have imported, split, and vectorized the data. Now we need to prepare it for a PyTorch model and proceed as we would for *any* classification task with a PyTorch model."],"metadata":{"id":"XGMs3RKZI8Of"}},{"cell_type":"markdown","source":["#### **Step #1**\n","\n","**This code has been provided for you. Run the cell below.**"],"metadata":{"id":"eN8Z-C_3K0ZG"}},{"cell_type":"code","source":["# Convert to PyTorch tensors\n","X_train = torch.tensor(X_train_bow.todense()).float()\n","X_valid = torch.tensor(X_valid_bow.todense()).float()\n","\n","# Extract labels\n","y_train = torch.tensor(labels_train)\n","y_valid = torch.tensor(labels_val)\n","\n","# Create DataLoaders\n","train_dataset = list(zip(X_train, y_train))\n","valid_dataset = list(zip(X_valid, y_valid))\n","\n","train_dl = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","val_dl = DataLoader(valid_dataset, batch_size=64)\n","dls = DataLoaders(train_dl,val_dl)"],"metadata":{"id":"liLaJe3dLPN9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Step #2**\n","\n","For a fully connected network, the dimension of the input layer will be the number of tokens. Complete the code below."],"metadata":{"id":"hc5z8AhGCKHt"}},{"cell_type":"code","source":[],"metadata":{"id":"A4H2ukzNCj41"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Steps #3-6**\n","\n","Define a fully connected network of your own design. Ensure you have the correct number of inputs and outputs."],"metadata":{"id":"ZbhgBwEPCm8P"}},{"cell_type":"code","source":[],"metadata":{"id":"qrIklOq1CmS0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Step #7**\n","\n","Create a Learner object and fit the model. Since this is a multiclass classification problem, you will use `nn.CrossEntropyLoss()`"],"metadata":{"id":"fuGPjq1bLSNA"}},{"cell_type":"code","source":["# Create a Learner and train the model\n"],"metadata":{"id":"rjiBv6JzLk4C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Step #8**\n","\n","Now, evaluate the model for both the training and validation sets.\n"],"metadata":{"id":"JSOOynZqL0xo"}},{"cell_type":"code","source":["# Evaluate the training set\n","\n","\n","# Evaluate the test set\n"],"metadata":{"id":"HqPSJlsSL7_E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **How did your model perform?**"],"metadata":{"id":"zkn0RBS0DHPo"}},{"cell_type":"markdown","source":["<a name=\"p3\"></a>\n","\n","---\n","## **Part 3: News Group Classification with a CNN**\n","---\n","\n"],"metadata":{"id":"r4_U4kWylKLw"}},{"cell_type":"markdown","source":["#### **Step #1**\n","\n","**The code for importing the data is provided for you. Run the cell below.**"],"metadata":{"id":"BQ6jclf5Po-o"}},{"cell_type":"code","source":["# Convert to PyTorch tensors\n","X_train = torch.tensor(X_train_bow.todense()).float().unsqueeze(1)\n","X_valid = torch.tensor(X_valid_bow.todense()).float().unsqueeze(1)\n","\n","# Extract labels\n","y_train = torch.tensor(labels_train)\n","y_valid = torch.tensor(labels_val)\n","\n","# Create DataLoaders\n","train_dataset = list(zip(X_train, y_train))\n","valid_dataset = list(zip(X_valid, y_valid))\n","\n","train_dl = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","val_dl = DataLoader(valid_dataset, batch_size=64)\n","dls = DataLoaders(train_dl,val_dl)\n","\n","input_dims = len(vectorizer.get_feature_names_out())"],"metadata":{"id":"LY31FfYdP3xa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Steps #3-6**\n","\n","Let's start by building a new CNN model. Remember, the syntax for CNNs for NLP is a little different than for images. We will be using the 1D versions of the convolution and max pooling layers. Examples:\n","* `nn.Conv1d(64, 128, kernel_size=5, padding=2)`\n","* `nn.MaxPool1d(2)`\n","\n","Define a CNN with the following layers:\n","\n","Block 1:\n","* A convolutional layer with the appropriate input dimension and 16 outputs, kernel size of 3, `padding=1`, and ReLU activation.\n","* A max pooling layer with a pool size of 2\n","\n","Block 2\n","* A convolutional layer with 32 outputs, kernel size of 3, `padding=1`, and ReLU activation.\n","* A max pooling layer with a pool size of 2\n","\n","Finally, add:\n","* A linear layer with 8 outputs and ReLU activation\n","* The output layer"],"metadata":{"id":"1AAVJHkuE3nM"}},{"cell_type":"code","source":[],"metadata":{"id":"YVE6HlQ4E3nc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Step #7**\n","\n","Create a Learner object and fit the model. Since this is a multiclass classification problem, you will use `nn.CrossEntropyLoss()`"],"metadata":{"id":"M__-6lOuE3nc"}},{"cell_type":"code","source":["# Create a Learner and train the model\n"],"metadata":{"id":"OaB29Xv2E3nd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Step #8**\n","\n","Now, evaluate the model for both the training and validation sets.\n"],"metadata":{"id":"r5KifHJwE3ne"}},{"cell_type":"code","source":["# Evaluate the training set\n","\n","\n","# Evaluate the test set\n"],"metadata":{"id":"k_8YoV92E3ne"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Oh no!** It looks like the CNN didn't do much better! It turns out that tokenization and vectorization is not enough to prepare text data for deep learning. There's an additional processing step we can take that will set our models up for success: **embedding.** We will see how embedding improves model performance in the next lab."],"metadata":{"id":"S6ddmcs4xIKQ"}},{"cell_type":"markdown","source":["# End of notebook\n","---\n","Â© 2024 The Coding School, All rights reserved"],"metadata":{"id":"7dzC09dLlEhm"}}]}